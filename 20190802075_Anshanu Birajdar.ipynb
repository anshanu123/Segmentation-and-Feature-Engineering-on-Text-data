{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Anshanu Birajdar\n",
        "\n",
        "PRN: 20190802075\n",
        "\n",
        "Aim: To perform segmentation and Feature Engineering on Text data."
      ],
      "metadata": {
        "id": "dJ1wYnaG6LPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('webtext')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgIUReT_0UFS",
        "outputId": "d3f30590-d2f3-4ecc-e5b7-a80c632ea5e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import requests\n",
        "import json \n",
        "file1 = open('/content/3.pdf', 'rb')\n",
        "pdfReader = PyPDF2.PdfFileReader(file1)\n",
        "print(\"Total Pages : {} \".format(pdfReader.numPages))\n",
        "for i in range(pdfReader.getNumPages()):\n",
        "  page = pdfReader.getPage(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM4SqdvQ1cPv",
        "outputId": "6b9ce5e7-30d0-4ea9-e7b0-33391569c4d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Pages : 14 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(pdfReader.getNumPages()):\n",
        "  page = pdfReader.getPage(i)\n",
        "  p1 = str(1+pdfReader.getPageNumber(page))\n",
        "  print(\"Page No.:\" + p1)\n",
        "  pageContent = page.extractText()\n",
        "  print(pageContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTcxtQKi1wlJ",
        "outputId": "037fb650-b9fd-4e9a-fc70-dc03235dd1e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page No.:1\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "Page No.:2\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "2\n",
            "multiple scaled imagesmultiple filter sizes\n",
            "multiple references\n",
            "(a) (b) (c)imagefeature map\n",
            "imagefeature map\n",
            "imagefeature map\n",
            "Figure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps\n",
            "are built, and the classiﬁer is run at all scales. (b) Pyramids of ﬁlters with multiple scales/sizes are run on\n",
            "the feature map. (c) We use pyramids of reference boxes in the regression functions.\n",
            "pyramids of images (Figure 1, a) or pyramids of ﬁlters 59\n",
            "(Figure 1, b), we introduce novel “anchor” boxes 60\n",
            "that serve as references at multiple scales and aspect 61\n",
            "ratios. Our scheme can be thought of as a pyramid 62\n",
            "of regression references (Figure 1, c), which avoids 63\n",
            "enumerating images or ﬁlters of multiple scales or 64\n",
            "aspect ratios. This model performs well when trained 65\n",
            "and tested using single-scale images and thus beneﬁts 66\n",
            "running speed. 67\n",
            "To unify RPNs with Fast R-CNN [2] object detec- 68\n",
            "tion networks, we propose a training scheme that 69\n",
            "alternates between ﬁne-tuning for the region proposal 70\n",
            "task and then ﬁne-tuning for object detection, while 71\n",
            "keeping the proposals ﬁxed. This scheme converges 72\n",
            "quickly and produces a uniﬁed network with convo- 73\n",
            "lutional features that are shared between both tasks.174\n",
            "We comprehensively evaluate our method on the 75\n",
            "PASCAL VOC detection benchmarks [11] where RPNs 76\n",
            "with Fast R-CNNs produce detection accuracy bet- 77\n",
            "ter than the strong baseline of Selective Search with 78\n",
            "Fast R-CNNs. Meanwhile, our method waives nearly 79\n",
            "all computational burdens of Selective Search at 80\n",
            "test-time—the effective running time for proposals 81\n",
            "is just 10 milliseconds. Using the expensive very 82\n",
            "deep models of [3], our detection method still has 83\n",
            "a frame rate of 5fps (including all steps) on a GPU, 84\n",
            "and thus is a practical object detection system in 85\n",
            "terms of both speed and accuracy. We also report 86\n",
            "results on the MS COCO dataset [12] and investi- 87\n",
            "gate the improvements on PASCAL VOC using the 88\n",
            "COCO data. Code has been made publicly available 89\n",
            "athttps://github.com/shaoqingren/faster_ 90\n",
            "rcnn (in MATLAB) and https://github.com/ 91\n",
            "rbgirshick/py-faster-rcnn (in Python). 92\n",
            "A preliminary version of this manuscript was pub- 93\n",
            "lished previously [10]. Since then, the frameworks of 94\n",
            "RPN and Faster R-CNN have been adopted and gen- 95\n",
            "eralized to other methods, such as 3D object detection 96\n",
            "[13], part-based detection [14], instance segmentation 97\n",
            "[15], and image captioning [16]. Our fast and effective 98\n",
            "object detection system has also been built in com- 99\n",
            "mercial systems such as at Pinterests [17], with user 100\n",
            "1. Since the publication of the conference version of this paper\n",
            "[10], we have also found that RPNs can be trained jointly with Fast\n",
            "R-CNN networks leading to less training time.engagement improvements reported. 101\n",
            "In ILSVRC and COCO 2015 competitions, Faster 102\n",
            "R-CNN and RPN are the basis of several 1st-place 103\n",
            "entries [18] in the tracks of ImageNet detection, Ima- 104\n",
            "geNet localization, COCO detection, and COCO seg- 105\n",
            "mentation. RPNs completely learn to propose regions 106\n",
            "from data, and thus can easily beneﬁt from deeper 107\n",
            "and more expressive features (such as the 101-layer 108\n",
            "residual nets adopted in [18]). Faster R-CNN and RPN 109\n",
            "are also used by several other leading entries in these 110\n",
            "competitions2. These results suggest that our method 111\n",
            "is not only a cost-efﬁcient solution for practical usage, 112\n",
            "but also an effective way of improving object detec- 113\n",
            "tion accuracy. 114\n",
            "2 R ELATED WORK 115\n",
            "Object Proposals. There is a large literature on object 116\n",
            "proposal methods. Comprehensive surveys and com- 117\n",
            "parisons of object proposal methods can be found in 118\n",
            "[19], [20], [21]. Widely used object proposal methods 119\n",
            "include those based on grouping super-pixels (e.g., 120\n",
            "Selective Search [4], CPMC [22], MCG [23]) and those 121\n",
            "based on sliding windows (e.g., objectness in windows 122\n",
            "[24], EdgeBoxes [6]). Object proposal methods were 123\n",
            "adopted as external modules independent of the de- 124\n",
            "tectors (e.g., Selective Search [4] object detectors, R- 125\n",
            "CNN [5], and Fast R-CNN [2]). 126\n",
            "Deep Networks for Object Detection. The R-CNN 127\n",
            "method [5] trains CNNs end-to-end to classify the 128\n",
            "proposal regions into object categories or background. 129\n",
            "R-CNN mainly plays as a classiﬁer, and it does not 130\n",
            "predict object bounds (except for reﬁning by bounding 131\n",
            "box regression). Its accuracy depends on the perfor- 132\n",
            "mance of the region proposal module (see compar- 133\n",
            "isons in [20]). Several papers have proposed ways of 134\n",
            "using deep networks for predicting object bounding 135\n",
            "boxes [25], [9], [26], [27]. In the OverFeat method [9], 136\n",
            "a fully-connected layer is trained to predict the box 137\n",
            "coordinates for the localization task that assumes a 138\n",
            "single object. The fully-connected layer is then turned 139\n",
            "into a convolutional layer for detecting multiple class- 140\n",
            "speciﬁc objects. The MultiBox methods [26], [27] gen- 141\n",
            "erate region proposals from a network whose last 142\n",
            "2. http://image-net.org/challenges/LSVRC/2015/results\n",
            "Page No.:3\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "3\n",
            "imageconv layers\n",
            "feature mapsRegion Proposal Networkproposals\n",
            "classifier\n",
            "RoI pooling\n",
            "Figure 2: Faster R-CNN is a single, uniﬁed network\n",
            "for object detection. The RPN module serves as the\n",
            "‘attention’ of this uniﬁed network.\n",
            "fully-connected layer simultaneously predicts mul- 143\n",
            "tiple class-agnostic boxes, generalizing the “single- 144\n",
            "box” fashion of OverFeat. These class-agnostic boxes 145\n",
            "are used as proposals for R-CNN [5]. The MultiBox 146\n",
            "proposal network is applied on a single image crop or 147\n",
            "multiple large image crops (e.g., 224\u0002224), in contrast 148\n",
            "to our fully convolutional scheme. MultiBox does not 149\n",
            "share features between the proposal and detection 150\n",
            "networks. We discuss OverFeat and MultiBox in more 151\n",
            "depth later in context with our method. Concurrent 152\n",
            "with our work, the DeepMask method [28] is devel- 153\n",
            "oped for learning segmentation proposals. 154\n",
            "Shared computation of convolutions [9], [1], [29], 155\n",
            "[7], [2] has been attracting increasing attention for ef- 156\n",
            "ﬁcient, yet accurate, visual recognition. The OverFeat 157\n",
            "paper [9] computes convolutional features from an 158\n",
            "image pyramid for classiﬁcation, localization, and de- 159\n",
            "tection. Adaptively-sized pooling (SPP) [1] on shared 160\n",
            "convolutional feature maps is developed for efﬁcient 161\n",
            "region-based object detection [1], [30] and semantic 162\n",
            "segmentation [29]. Fast R-CNN [2] enables end-to-end 163\n",
            "detector training on shared convolutional features and 164\n",
            "shows compelling accuracy and speed. 165\n",
            "3 F ASTER R-CNN 166\n",
            "Our object detection system, called Faster R-CNN, is 167\n",
            "composed of two modules. The ﬁrst module is a deep 168\n",
            "fully convolutional network that proposes regions, 169\n",
            "and the second module is the Fast R-CNN detector [2] 170\n",
            "that uses the proposed regions. The entire system is a 171\n",
            "single, uniﬁed network for object detection (Figure 2). 172\n",
            "Using the recently popular terminology of neural 173\n",
            "networks with ‘attention’ [31] mechanisms, the RPN 174\n",
            "module tells the Fast R-CNN module where to look. 175\n",
            "In Section 3.1 we introduce the designs and properties 176\n",
            "of the network for region proposal. In Section 3.2 we 177develop algorithms for training both modules with 178\n",
            "features shared. 179\n",
            "3.1 Region Proposal Networks 180\n",
            "A Region Proposal Network (RPN) takes an image 181\n",
            "(of any size) as input and outputs a set of rectangular 182\n",
            "object proposals, each with an objectness score.3We 183\n",
            "model this process with a fully convolutional network 184\n",
            "[7], which we describe in this section. Because our ulti- 185\n",
            "mate goal is to share computation with a Fast R-CNN 186\n",
            "object detection network [2], we assume that both nets 187\n",
            "share a common set of convolutional layers. In our ex- 188\n",
            "periments, we investigate the Zeiler and Fergus model 189\n",
            "[32] (ZF), which has 5 shareable convolutional layers 190\n",
            "and the Simonyan and Zisserman model [3] (VGG-16), 191\n",
            "which has 13 shareable convolutional layers. 192\n",
            "To generate region proposals, we slide a small 193\n",
            "network over the convolutional feature map output 194\n",
            "by the last shared convolutional layer. This small 195\n",
            "network takes as input an n\u0002nspatial window of 196\n",
            "the input convolutional feature map. Each sliding 197\n",
            "window is mapped to a lower-dimensional feature 198\n",
            "(256-d for ZF and 512-d for VGG, with ReLU [33] 199\n",
            "following). This feature is fed into two sibling fully- 200\n",
            "connected layers—a box-regression layer (reg) and a 201\n",
            "box-classiﬁcation layer (cls). We use n= 3 in this 202\n",
            "paper, noting that the effective receptive ﬁeld on the 203\n",
            "input image is large (171 and 228 pixels for ZF and 204\n",
            "VGG, respectively). This mini-network is illustrated 205\n",
            "at a single position in Figure 3 (left). Note that be- 206\n",
            "cause the mini-network operates in a sliding-window 207\n",
            "fashion, the fully-connected layers are shared across 208\n",
            "all spatial locations. This architecture is naturally im- 209\n",
            "plemented with an n\u0002nconvolutional layer followed 210\n",
            "by two sibling 1\u00021convolutional layers (for regand 211\n",
            "cls, respectively). 212\n",
            "3.1.1 Anchors 213\n",
            "At each sliding-window location, we simultaneously 214\n",
            "predict multiple region proposals, where the number 215\n",
            "of maximum possible proposals for each location is 216\n",
            "denoted as k. So the reglayer has 4koutputs encoding 217\n",
            "the coordinates of kboxes, and the clslayer outputs 218\n",
            "2kscores that estimate probability of object or not 219\n",
            "object for each proposal4. Thekproposals are param- 220\n",
            "eterized relative tokreference boxes, which we call 221\n",
            "anchors. An anchor is centered at the sliding window 222\n",
            "in question, and is associated with a scale and aspect 223\n",
            "ratio (Figure 3, left). By default we use 3 scales and 224\n",
            "3 aspect ratios, yielding k= 9 anchors at each sliding 225\n",
            "position. For a convolutional feature map of a size 226\n",
            "3. “Region” is a generic term and in this paper we only consider\n",
            "rectangular regions, as is common for many methods ( e.g., [27], [4],\n",
            "[6]). “Objectness” measures membership to a set of object classes\n",
            "vs. background.\n",
            "4. For simplicity we implement the clslayer as a two-class\n",
            "softmax layer. Alternatively, one may use logistic regression to\n",
            "produce kscores.\n",
            "Page No.:4\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "4\n",
            "car : 1.000\n",
            "dog : 0.997person : 0.992\n",
            "person : 0.979horse : 0.993\n",
            "conv feature mapintermediate layer256-d2kscores 4kcoordinates\n",
            "sliding windowreglayer clslayerkanchorboxes\n",
            "bus : 0.996\n",
            "person : 0.736\n",
            "boat : 0.970\n",
            "person : 0.989person : 0.983person : 0.983\n",
            "person : 0.925\n",
            "cat : 0.982dog : 0.994\n",
            "Figure 3: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals on PASCAL\n",
            "VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\n",
            "W\u0002H(typically\u00182,400), there are WHk anchors in 227\n",
            "total. 228\n",
            "Translation-Invariant Anchors 229\n",
            "An important property of our approach is that it 230\n",
            "istranslation invariant, both in terms of the anchors 231\n",
            "and the functions that compute proposals relative to 232\n",
            "the anchors. If one translates an object in an image, 233\n",
            "the proposal should translate and the same function 234\n",
            "should be able to predict the proposal in either lo- 235\n",
            "cation. This translation-invariant property is guaran- 236\n",
            "teed by our method5. As a comparison, the MultiBox 237\n",
            "method [27] uses k-means to generate 800 anchors, 238\n",
            "which are nottranslation invariant. So MultiBox does 239\n",
            "not guarantee that the same proposal is generated if 240\n",
            "an object is translated. 241\n",
            "The translation-invariant property also reduces the 242\n",
            "model size. MultiBox has a (4 + 1)\u0002800-dimensional 243\n",
            "fully-connected output layer, whereas our method has 244\n",
            "a(4 + 2)\u00029-dimensional convolutional output layer 245\n",
            "in the case of k= 9 anchors6. As a result, our output 246\n",
            "layer has 2:8\u0002104parameters (512 \u0002(4 + 2)\u00029 247\n",
            "for VGG-16), two orders of magnitude fewer than 248\n",
            "MultiBox’s output layer that has 6:1\u0002106parameters 249\n",
            "(1536\u0002(4 + 1)\u0002800for GoogleNet [34] in MultiBox 250\n",
            "[27]). If considering the feature projection layers, our 251\n",
            "proposal layers still have an order of magnitude fewer 252\n",
            "parameters than MultiBox7. We expect our method 253\n",
            "to have less risk of overﬁtting on small datasets, like 254\n",
            "PASCAL VOC. 255\n",
            "Multi-Scale Anchors as Regression References 256\n",
            "Our design of anchors presents a novel scheme 257\n",
            "for addressing multiple scales (and aspect ratios). As 258\n",
            "shown in Figure 1, there have been two popular ways 259\n",
            "for multi-scale predictions. The ﬁrst way is based on 260\n",
            "5. As is the case of FCNs [7], our network is translation invariant\n",
            "up to the network’s total stride.\n",
            "6. 4 is the dimension of regterm for each category, and 1 or 2 is\n",
            "the dimension of clsterm of sigmoid or softmax for each category\n",
            "7. Considering the feature projection layers, our proposal layers’\n",
            "parameter count is 3\u00023\u0002512\u0002512 + 512\u00026\u00029 = 2:4\u0002106;\n",
            "MultiBox’s proposal layers’ parameter count is 7\u00027\u0002(64 + 96 +\n",
            "64 + 64)\u00021536 + 1536\u00025\u0002800 = 27\u0002106.image/feature pyramids, e.g., in DPM [8] and CNN- 261\n",
            "based methods [9], [1], [2]. The images are resized at 262\n",
            "multiple scales, and feature maps (HOG [8] or deep 263\n",
            "convolutional features [9], [1], [2]) are computed for 264\n",
            "each scale (Figure 1(a)). This way is often useful but 265\n",
            "is time-consuming. The second way is to use sliding 266\n",
            "windows of multiple scales (and/or aspect ratios) on 267\n",
            "the feature maps. For example, in DPM [8], models 268\n",
            "of different aspect ratios are trained separately using 269\n",
            "different ﬁlter sizes (such as 5\u00027 and 7\u00025). If this way 270\n",
            "is used to address multiple scales, it can be thought 271\n",
            "of as a “pyramid of ﬁlters” (Figure 1(b)). The second 272\n",
            "way is usually adopted jointly with the ﬁrst way [8]. 273\n",
            "As a comparison, our anchor-based method is built 274\n",
            "ona pyramid of anchors, which is more cost-efﬁcient. 275\n",
            "Our method classiﬁes and regresses bounding boxes 276\n",
            "with reference to anchor boxes of multiple scales and 277\n",
            "aspect ratios. It only relies on images and feature 278\n",
            "maps of a single scale, and uses ﬁlters (sliding win- 279\n",
            "dows on the feature map) of a single size. We show by 280\n",
            "experiments the effects of this scheme for addressing 281\n",
            "multiple scales and sizes (Table 8). 282\n",
            "Because of this multi-scale design based on anchors, 283\n",
            "we can simply use the convolutional features com- 284\n",
            "puted on a single-scale image, as is also done by 285\n",
            "the Fast R-CNN detector [2]. The design of multi- 286\n",
            "scale anchors is a key component for sharing features 287\n",
            "without extra cost for addressing scales. 288\n",
            "3.1.2 Loss Function 289\n",
            "For training RPNs, we assign a binary class label 290\n",
            "(of being an object or not) to each anchor. We as- 291\n",
            "sign a positive label to two kinds of anchors: (i) the 292\n",
            "anchor/anchors with the highest Intersection-over- 293\n",
            "Union (IoU) overlap with a ground-truth box, or(ii) an 294\n",
            "anchor that has an IoU overlap higher than 0.7 with 295\n",
            "any ground-truth box. Note that a single ground-truth 296\n",
            "box may assign positive labels to multiple anchors. 297\n",
            "Usually the second condition is sufﬁcient to determine 298\n",
            "the positive samples; but we still adopt the ﬁrst 299\n",
            "condition for the reason that in some rare cases the 300\n",
            "second condition may ﬁnd no positive sample. We 301\n",
            "Page No.:5\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "5\n",
            "assign a negative label to a non-positive anchor if its 302\n",
            "IoU ratio is lower than 0.3 for all ground-truth boxes. 303\n",
            "Anchors that are neither positive nor negative do not 304\n",
            "contribute to the training objective. 305\n",
            "With these deﬁnitions, we minimize an objective 306\n",
            "function following the multi-task loss in Fast R-CNN 307\n",
            "[2]. Our loss function for an image is deﬁned as: 308\n",
            "L(fp ig;ftig) =1\n",
            "NclsX\n",
            "iLcls(pi;p\u0003\n",
            "i)\n",
            "+\u00151\n",
            "NregX\n",
            "ip\u0003\n",
            "iLreg(ti;t\u0003\n",
            "i):(1)\n",
            "Here,iis the index of an anchor in a mini-batch and 309\n",
            "piis the predicted probability of anchor ibeing an 310\n",
            "object. The ground-truth label p\u0003\n",
            "iis 1 if the anchor 311\n",
            "is positive, and is 0 if the anchor is negative. tiis a 312\n",
            "vector representing the 4 parameterized coordinates 313\n",
            "of the predicted bounding box, and t\u0003\n",
            "iis that of the 314\n",
            "ground-truth box associated with a positive anchor. 315\n",
            "The classiﬁcation loss Lclsis log loss over two classes 316\n",
            "(object vs. not object). For the regression loss, we use 317\n",
            "Lreg(ti;t\u0003\n",
            "i) =R(ti\u0000t\u0003\n",
            "i)whereRis the robust loss 318\n",
            "function (smooth L 1) deﬁned in [2]. The term p\u0003\n",
            "iLreg 319\n",
            "means the regression loss is activated only for positive 320\n",
            "anchors (p\u0003\n",
            "i= 1) and is disabled otherwise (p\u0003\n",
            "i= 0). 321\n",
            "The outputs of the clsand reglayers consist of fpig 322\n",
            "andftigrespectively. 323\n",
            "The two terms are normalized by NclsandNreg 324\n",
            "and weighted by a balancing parameter \u0015. In our 325\n",
            "current implementation (as in the released code), the 326\n",
            "clsterm in Eqn.(1) is normalized by the mini-batch 327\n",
            "size (i.e.,Ncls= 256) and the regterm is normalized 328\n",
            "by the number of anchor locations (i.e., Nreg\u00182;400). 329\n",
            "By default we set \u0015= 10, and thus both clsand 330\n",
            "regterms are roughly equally weighted. We show 331\n",
            "by experiments that the results are insensitive to the 332\n",
            "values of\u0015in a wide range (Table 9). We also note 333\n",
            "that the normalization as above is not required and 334\n",
            "could be simpliﬁed. 335\n",
            "For bounding box regression, we adopt the param- 336\n",
            "eterizations of the 4 coordinates following [5]: 337\n",
            "tx= (x\u0000xa)=w a; t y= (y\u0000ya)=h a;\n",
            "tw= log(w=w a); t h= log(h=h a);\n",
            "t\u0003\n",
            "x= (x\u0003\u0000xa)=w a; t\u0003\n",
            "y= (y\u0003\u0000ya)=h a;\n",
            "t\u0003\n",
            "w= log(w\u0003=wa); t\u0003\n",
            "h= log(h\u0003=ha);(2)\n",
            "wherex,y,w, andhdenote the box’s center coordi- 338\n",
            "nates and its width and height. Variables x,xa, and 339\n",
            "x\u0003are for the predicted box, anchor box, and ground- 340\n",
            "truth box respectively (likewise for y;w;h). This can 341\n",
            "be thought of as bounding-box regression from an 342\n",
            "anchor box to a nearby ground-truth box. 343\n",
            "Nevertheless, our method achieves bounding-box 344\n",
            "regression by a different manner from previous RoI- 345\n",
            "based (Region of Interest) methods [1], [2]. In [1], 346\n",
            "[2], bounding-box regression is performed on features 347pooled from arbitrarily sized RoIs, and the regression 348\n",
            "weights are shared by all region sizes. In our formula- 349\n",
            "tion, the features used for regression are of the same 350\n",
            "spatial size (3\u00023) on the feature maps. To account 351\n",
            "for varying sizes, a set of kbounding-box regressors 352\n",
            "are learned. Each regressor is responsible for one scale 353\n",
            "and one aspect ratio, and the kregressors do notshare 354\n",
            "weights. As such, it is still possible to predict boxes of 355\n",
            "various sizes even though the features are of a ﬁxed 356\n",
            "size/scale, thanks to the design of anchors. 357\n",
            "3.1.3 Training RPNs 358\n",
            "The RPN can be trained end-to-end by back- 359\n",
            "propagation and stochastic gradient descent (SGD) 360\n",
            "[35]. We follow the “image-centric” sampling strategy 361\n",
            "from [2] to train this network. Each mini-batch arises 362\n",
            "from a single image that contains many positive and 363\n",
            "negative example anchors. It is possible to optimize 364\n",
            "for the loss functions of all anchors, but this will 365\n",
            "bias towards negative samples as they are dominate. 366\n",
            "Instead, we randomly sample 256 anchors in an image 367\n",
            "to compute the loss function of a mini-batch, where 368\n",
            "the sampled positive and negative anchors have a 369\n",
            "ratio of up to 1:1. If there are fewer than 128 positive 370\n",
            "samples in an image, we pad the mini-batch with 371\n",
            "negative ones. 372\n",
            "We randomly initialize all new layers by drawing 373\n",
            "weights from a zero-mean Gaussian distribution with 374\n",
            "standard deviation 0.01. All other layers (i.e., the 375\n",
            "shared convolutional layers) are initialized by pre- 376\n",
            "training a model for ImageNet classiﬁcation [36], as 377\n",
            "is standard practice [5]. We tune all layers of the 378\n",
            "ZF net, and conv3 1and up for the VGG net to 379\n",
            "conserve memory [2]. We use a learning rate of 0.001 380\n",
            "for 60k mini-batches, and 0.0001 for the next 20k 381\n",
            "mini-batches on the PASCAL VOC dataset. We use a 382\n",
            "momentum of 0.9 and a weight decay of 0.0005 [37]. 383\n",
            "Our implementation uses Caffe [38]. 384\n",
            "3.2 Sharing Features for RPN and Fast R-CNN 385\n",
            "Thus far we have described how to train a network 386\n",
            "for region proposal generation, without considering 387\n",
            "the region-based object detection CNN that will utilize 388\n",
            "these proposals. For the detection network, we adopt 389\n",
            "Fast R-CNN [2]. Next we describe algorithms that 390\n",
            "learn a uniﬁed network composed of RPN and Fast 391\n",
            "R-CNN with shared convolutional layers (Figure 2). 392\n",
            "Both RPN and Fast R-CNN, trained independently, 393\n",
            "will modify their convolutional layers in different 394\n",
            "ways. We therefore need to develop a technique that 395\n",
            "allows for sharing convolutional layers between the 396\n",
            "two networks, rather than learning two separate net- 397\n",
            "works. We discuss three ways for training networks 398\n",
            "with features shared: 399\n",
            "(i)Alternating training. In this solution, we ﬁrst train 400\n",
            "RPN, and use the proposals to train Fast R-CNN. 401\n",
            "The network tuned by Fast R-CNN is then used to 402\n",
            "Page No.:6\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "6\n",
            "Table 1: the learned average proposal size for each anchor using the ZF net (numbers for s= 600).\n",
            "anchor 1282, 2:1 1282, 1:1 1282, 1:2 2562, 2:1 2562, 1:1 2562, 1:2 5122, 2:1 5122, 1:1 5122, 1:2\n",
            "proposal 188\u0002111 113\u0002114 70\u000292 416\u0002229 261\u0002284 174\u0002332 768\u0002437 499\u0002501 355\u0002715\n",
            "initialize RPN, and this process is iterated. This is the 403\n",
            "solution that is used in all experiments in this paper. 404\n",
            "(ii)Approximate joint training. In this solution, the 405\n",
            "RPN and Fast R-CNN networks are merged into one 406\n",
            "network during training as in Figure 2. In each SGD 407\n",
            "iteration, the forward pass generates region propos- 408\n",
            "als which are treated just like ﬁxed, pre-computed 409\n",
            "proposals when training a Fast R-CNN detector. The 410\n",
            "backward propagation takes place as usual, where for 411\n",
            "the shared layers the backward propagated signals 412\n",
            "from both the RPN loss and the Fast R-CNN loss 413\n",
            "are combined. This solution is easy to implement. But 414\n",
            "this solution ignores the derivative w.r.t. the proposal 415\n",
            "boxes’ coordinates that are also network responses, so 416\n",
            "is approximate. In our experiments, we have empiri- 417\n",
            "cally found this solver produces close results (mAP 418\n",
            "70.0% compared with 69.9% of alternating training 419\n",
            "reported in Table 3), yet reduces the training time 420\n",
            "by about 25-50% comparing with alternating training. 421\n",
            "This solver is included in our released Python code. 422\n",
            "(iii) Non-approximate joint training. As discussed 423\n",
            "above, the bounding boxes predicted by RPN are 424\n",
            "also functions of the input. The RoI pooling layer 425\n",
            "[2] in Fast R-CNN accepts the convolutional features 426\n",
            "and also the predicted bounding boxes as input, so 427\n",
            "a theoretically valid backpropagation solver should 428\n",
            "also involve gradients w.r.t. the box coordinates. These 429\n",
            "gradients are ignored in the above approximate joint 430\n",
            "training. In a non-approximate joint training solution, 431\n",
            "we need an RoI pooling layer that is differentiable 432\n",
            "w.r.t. the box coordinates. This is a nontrivial problem 433\n",
            "and a solution can be given by an “RoI warping” layer 434\n",
            "as developed in [15], which is beyond the scope of this 435\n",
            "paper. 436\n",
            "4-Step Alternating Training. In this paper, we adopt 437\n",
            "a pragmatic 4-step training algorithm to learn shared 438\n",
            "features via alternating optimization. In the ﬁrst step, 439\n",
            "we train the RPN as described in Section 3.1.3. This 440\n",
            "network is initialized with an ImageNet-pre-trained 441\n",
            "model and ﬁne-tuned end-to-end for the region pro- 442\n",
            "posal task. In the second step, we train a separate 443\n",
            "detection network by Fast R-CNN using the proposals 444\n",
            "generated by the step-1 RPN. This detection net- 445\n",
            "work is also initialized by the ImageNet-pre-trained 446\n",
            "model. At this point the two networks do not share 447\n",
            "convolutional layers. In the third step, we use the 448\n",
            "detector network to initialize RPN training, but we 449\n",
            "ﬁx the shared convolutional layers and only ﬁne-tune 450\n",
            "the layers unique to RPN. Now the two networks 451\n",
            "share convolutional layers. Finally, keeping the shared 452\n",
            "convolutional layers ﬁxed, we ﬁne-tune the unique 453\n",
            "layers of Fast R-CNN. As such, both networks share 454\n",
            "the same convolutional layers and form a uniﬁed 455network. A similar alternating training can be run 456\n",
            "for more iterations, but we have observed negligible 457\n",
            "improvements. 458\n",
            "3.3 Implementation Details 459\n",
            "We train and test both region proposal and object 460\n",
            "detection networks on images of a single scale [1], [2]. 461\n",
            "We re-scale the images such that their shorter side 462\n",
            "iss= 600 pixels [2]. Multi-scale feature extraction 463\n",
            "(using an image pyramid) may improve accuracy but 464\n",
            "does not exhibit a good speed-accuracy trade-off [2]. 465\n",
            "On the re-scaled images, the total stride for both ZF 466\n",
            "and VGG nets on the last convolutional layer is 16 467\n",
            "pixels, and thus is \u001810 pixels on a typical PASCAL 468\n",
            "image before resizing (\u0018500\u0002375). Even such a large 469\n",
            "stride provides good results, though accuracy may be 470\n",
            "further improved with a smaller stride. 471\n",
            "For anchors, we use 3 scales with box areas of 1282, 472\n",
            "2562, and 5122pixels, and 3 aspect ratios of 1:1, 1:2, 473\n",
            "and 2:1. These hyper-parameters are notcarefully cho- 474\n",
            "sen for a particular dataset, and we provide ablation 475\n",
            "experiments on their effects in the next section. As dis- 476\n",
            "cussed, our solution does not need an image pyramid 477\n",
            "or ﬁlter pyramid to predict regions of multiple scales, 478\n",
            "saving considerable running time. Figure 3 (right) 479\n",
            "shows the capability of our method for a wide range 480\n",
            "of scales and aspect ratios. Table 1 shows the learned 481\n",
            "average proposal size for each anchor using the ZF 482\n",
            "net. We note that our algorithm allows predictions 483\n",
            "that are larger than the underlying receptive ﬁeld. 484\n",
            "Such predictions are not impossible—one may still 485\n",
            "roughly infer the extent of an object if only the middle 486\n",
            "of the object is visible. 487\n",
            "The anchor boxes that cross image boundaries need 488\n",
            "to be handled with care. During training, we ignore 489\n",
            "all cross-boundary anchors so they do not contribute 490\n",
            "to the loss. For a typical 1000\u0002600 image, there 491\n",
            "will be roughly 20000 (\u0019 60\u000240\u00029) anchors in 492\n",
            "total. With the cross-boundary anchors ignored, there 493\n",
            "are about 6000 anchors per image for training. If the 494\n",
            "boundary-crossing outliers are not ignored in training, 495\n",
            "they introduce large, difﬁcult to correct error terms in 496\n",
            "the objective, and training does not converge. During 497\n",
            "testing, however, we still apply the fully convolutional 498\n",
            "RPN to the entire image. This may generate cross- 499\n",
            "boundary proposal boxes, which we clip to the image 500\n",
            "boundary. 501\n",
            "Some RPN proposals highly overlap with each 502\n",
            "other. To reduce redundancy, we adopt non-maximum 503\n",
            "suppression (NMS) on the proposal regions based on 504\n",
            "their clsscores. We ﬁx the IoU threshold for NMS 505\n",
            "at 0.7, which leaves us about 2000 proposal regions 506\n",
            "per image. As we will show, NMS does not harm the 507\n",
            "ultimate detection accuracy, but substantially reduces 508\n",
            "Page No.:7\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "7\n",
            "Table 2: Detection results on PASCAL VOC 2007 test set (trained on VOC 2007 trainval). The detectors are\n",
            "Fast R-CNN with ZF, but using various proposal methods for training and testing.\n",
            "train-time region proposals test-time region proposals\n",
            "method # boxes method # proposals mAP (%)\n",
            "SS 2000 SS 2000 58.7\n",
            "EB 2000 EB 2000 58.6\n",
            "RPN+ZF, shared 2000 RPN+ZF, shared 300 59.9\n",
            "ablation experiments follow below\n",
            "RPN+ZF, unshared 2000 RPN+ZF, unshared 300 58.7\n",
            "SS 2000 RPN+ZF 100 55.1\n",
            "SS 2000 RPN+ZF 300 56.8\n",
            "SS 2000 RPN+ZF 1000 56.3\n",
            "SS 2000 RPN+ZF (no NMS) 6000 55.2\n",
            "SS 2000 RPN+ZF (no cls) 100 44.6\n",
            "SS 2000 RPN+ZF (no cls) 300 51.4\n",
            "SS 2000 RPN+ZF (no cls) 1000 55.8\n",
            "SS 2000 RPN+ZF (no reg) 300 52.1\n",
            "SS 2000 RPN+ZF (no reg) 1000 51.3\n",
            "SS 2000 RPN+VGG 300 59.2\n",
            "the number of proposals. After NMS, we use the 509\n",
            "top-N ranked proposal regions for detection. In the 510\n",
            "following, we train Fast R-CNN using 2000 RPN pro- 511\n",
            "posals, but evaluate different numbers of proposals at 512\n",
            "test-time. 513\n",
            "4 E XPERIMENTS 514\n",
            "4.1 Experiments on PASCAL VOC 515\n",
            "We comprehensively evaluate our method on the 516\n",
            "PASCAL VOC 2007 detection benchmark [11]. This 517\n",
            "dataset consists of about 5k trainval images and 5k 518\n",
            "test images over 20 object categories. We also provide 519\n",
            "results on the PASCAL VOC 2012 benchmark for a 520\n",
            "few models. For the ImageNet pre-trained network, 521\n",
            "we use the “fast” version of ZF net [32] that has 522\n",
            "5 convolutional layers and 3 fully-connected layers, 523\n",
            "and the public VGG-16 model8[3] that has 13 con- 524\n",
            "volutional layers and 3 fully-connected layers. We 525\n",
            "primarily evaluate detection mean Average Precision 526\n",
            "(mAP), because this is the actual metric for object 527\n",
            "detection (rather than focusing on object proposal 528\n",
            "proxy metrics). 529\n",
            "Table 2 (top) shows Fast R-CNN results when 530\n",
            "trained and tested using various region proposal 531\n",
            "methods. These results use the ZF net. For Selective 532\n",
            "Search (SS) [4], we generate about 2000 proposals by 533\n",
            "the “fast” mode. For EdgeBoxes (EB) [6], we generate 534\n",
            "the proposals by the default EB setting tuned for 0.7 535\n",
            "IoU. SS has an mAP of 58.7% and EB has an mAP 536\n",
            "of 58.6% under the Fast R-CNN framework. RPN 537\n",
            "with Fast R-CNN achieves competitive results, with 538\n",
            "an mAP of 59.9% while using up to 300 proposals9. 539\n",
            "Using RPN yields a much faster detection system than 540\n",
            "8. www.robots.ox.ac.uk/ \u0018vgg/research/very deep/\n",
            "9. For RPN, the number of proposals (e.g., 300) is the maximum\n",
            "number for an image. RPN may produce fewer proposals after\n",
            "NMS, and thus the average number of proposals is smaller.using either SS or EB because of shared convolutional 541\n",
            "computations; the fewer proposals also reduce the 542\n",
            "region-wise fully-connected layers’ cost (Table 5). 543\n",
            "Ablation Experiments on RPN. To investigate the be- 544\n",
            "havior of RPNs as a proposal method, we conducted 545\n",
            "several ablation studies. First, we show the effect of 546\n",
            "sharing convolutional layers between the RPN and 547\n",
            "Fast R-CNN detection network. To do this, we stop 548\n",
            "after the second step in the 4-step training process. 549\n",
            "Using separate networks reduces the result slightly to 550\n",
            "58.7% (RPN+ZF, unshared, Table 2). We observe that 551\n",
            "this is because in the third step when the detector- 552\n",
            "tuned features are used to ﬁne-tune the RPN, the 553\n",
            "proposal quality is improved. 554\n",
            "Next, we disentangle the RPN’s inﬂuence on train- 555\n",
            "ing the Fast R-CNN detection network. For this pur- 556\n",
            "pose, we train a Fast R-CNN model by using the 557\n",
            "2000 SS proposals and ZF net. We ﬁx this detector 558\n",
            "and evaluate the detection mAP by changing the 559\n",
            "proposal regions used at test-time. In these ablation 560\n",
            "experiments, the RPN does not share features with 561\n",
            "the detector. 562\n",
            "Replacing SS with 300 RPN proposals at test-time 563\n",
            "leads to an mAP of 56.8%. The loss in mAP is because 564\n",
            "of the inconsistency between the training/testing pro- 565\n",
            "posals. This result serves as the baseline for the fol- 566\n",
            "lowing comparisons. 567\n",
            "Somewhat surprisingly, the RPN still leads to a 568\n",
            "competitive result (55.1%) when using the top-ranked 569\n",
            "100 proposals at test-time, indicating that the top- 570\n",
            "ranked RPN proposals are accurate. On the other 571\n",
            "extreme, using the top-ranked 6000 RPN proposals 572\n",
            "(without NMS) has a comparable mAP (55.2%), sug- 573\n",
            "gesting NMS does not harm the detection mAP and 574\n",
            "may reduce false alarms. 575\n",
            "Next, we separately investigate the roles of RPN’s 576\n",
            "clsand regoutputs by turning off either of them 577\n",
            "at test-time. When the clslayer is removed at test- 578\n",
            "Page No.:8\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "8\n",
            "Table 3: Detection results on PASCAL VOC 2007 test set. The detector is Fast R-CNN and VGG-16. Training\n",
            "data: “07”: VOC 2007 trainval, “07+12”: union set of VOC 2007 trainval and VOC 2012 trainval. For RPN,\n",
            "the train-time proposals for Fast R-CNN are 2000.y: this number was reported in [2]; using the repository\n",
            "provided by this paper, this result is higher (68.1).\n",
            "method # proposals data mAP (%)\n",
            "SS 2000 07 66.9y\n",
            "SS 2000 07+12 70.0\n",
            "RPN+VGG, unshared 300 07 68.5\n",
            "RPN+VGG, shared 300 07 69.9\n",
            "RPN+VGG, shared 300 07+12 73.2\n",
            "RPN+VGG, shared 300 COCO+07+12 78.8\n",
            "Table 4: Detection results on PASCAL VOC 2012 test set. The detector is Fast R-CNN and VGG-16. Training\n",
            "data: “07”: VOC 2007 trainval, “07++12”: union set of VOC 2007 trainval+test and VOC 2012 trainval. For\n",
            "RPN, the train-time proposals for Fast R-CNN are 2000.y: http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html.z:\n",
            "http://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html.x: http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html.\n",
            "method # proposals data mAP (%)\n",
            "SS 2000 12 65.7\n",
            "SS 2000 07++12 68.4\n",
            "RPN+VGG, sharedy300 12 67.0\n",
            "RPN+VGG, sharedz300 07++12 70.4\n",
            "RPN+VGG, sharedx300 COCO+07++12 75.9\n",
            "Table 5: Timing (ms) on a K40 GPU, except SS proposal is evaluated in a CPU. “Region-wise” includes NMS,\n",
            "pooling, fully-connected, and softmax layers. See our released code for the proﬁling of running time.\n",
            "model system conv proposal region-wise total rate\n",
            "VGG SS + Fast R-CNN 146 1510 174 1830 0.5 fps\n",
            "VGG RPN + Fast R-CNN 141 10 47 198 5 fps\n",
            "ZF RPN + Fast R-CNN 31 3 25 59 17 fps\n",
            "time (thus no NMS/ranking is used), we randomly 579\n",
            "sampleNproposals from the unscored regions. The 580\n",
            "mAP is nearly unchanged with N= 1000 (55.8%), but 581\n",
            "degrades considerably to 44.6% when N= 100. This 582\n",
            "shows that the clsscores account for the accuracy of 583\n",
            "the highest ranked proposals. 584\n",
            "On the other hand, when the reglayer is removed 585\n",
            "at test-time (so the proposals become anchor boxes), 586\n",
            "the mAP drops to 52.1%. This suggests that the high- 587\n",
            "quality proposals are mainly due to the regressed box 588\n",
            "bounds. The anchor boxes, though having multiple 589\n",
            "scales and aspect ratios, are not sufﬁcient for accurate 590\n",
            "detection. 591\n",
            "We also evaluate the effects of more powerful net- 592\n",
            "works on the proposal quality of RPN alone. We use 593\n",
            "VGG-16 to train the RPN, and still use the above 594\n",
            "detector of SS+ZF. The mAP improves from 56.8% 595\n",
            "(using RPN+ZF) to 59.2% (using RPN+VGG). This is a 596\n",
            "promising result, because it suggests that the proposal 597\n",
            "quality of RPN+VGG is better than that of RPN+ZF. 598\n",
            "Because proposals of RPN+ZF are competitive with 599\n",
            "SS (both are 58.7% when consistently used for training 600\n",
            "and testing), we may expect RPN+VGG to be better 601\n",
            "than SS. The following experiments justify this hy- 602\n",
            "pothesis. 603\n",
            "Performance of VGG-16. Table 3 shows the results 604\n",
            "of VGG-16 for both proposal and detection. Using 605\n",
            "RPN+VGG, the result is 68.5% for unshared features, 606slightly higher than the SS baseline. As shown above, 607\n",
            "this is because the proposals generated by RPN+VGG 608\n",
            "are more accurate than SS. Unlike SS that is pre- 609\n",
            "deﬁned, the RPN is actively trained and beneﬁts from 610\n",
            "better networks. For the feature-shared variant, the 611\n",
            "result is 69.9%—better than the strong SS baseline, yet 612\n",
            "with nearly cost-free proposals. We further train the 613\n",
            "RPN and detection network on the union set of PAS- 614\n",
            "CAL VOC 2007 trainval and 2012 trainval. The mAP 615\n",
            "is73.2%. Figure 5 shows some results on the PASCAL 616\n",
            "VOC 2007 test set. On the PASCAL VOC 2012 test set 617\n",
            "(Table 4), our method has an mAP of 70.4% trained 618\n",
            "on the union set of VOC 2007 trainval+test and VOC 619\n",
            "2012 trainval. Table 6 and Table 7 show the detailed 620\n",
            "numbers. 621\n",
            "In Table 5 we summarize the running time of the 622\n",
            "entire object detection system. SS takes 1-2 seconds 623\n",
            "depending on content (on average about 1.5s), and 624\n",
            "Fast R-CNN with VGG-16 takes 320ms on 2000 SS 625\n",
            "proposals (or 223ms if using SVD on fully-connected 626\n",
            "layers [2]). Our system with VGG-16 takes in total 627\n",
            "198ms for both proposal and detection. With the con- 628\n",
            "volutional features shared, the RPN alone only takes 629\n",
            "10ms computing the additional layers. Our region- 630\n",
            "wise computation is also lower, thanks to fewer pro- 631\n",
            "posals (300 per image). Our system has a frame-rate 632\n",
            "of 17 fps with the ZF net. 633\n",
            "Sensitivities to Hyper-parameters. In Table 8 we 634\n",
            "Page No.:9\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "9\n",
            "Table 6: Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time\n",
            "proposals for Fast R-CNN are 2000. RPN\u0003denotes the unsharing feature version.\n",
            "method #\n",
            "box data mAP ar\n",
            "eo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\n",
            "SS 2000 07 66.9 74.5\n",
            "78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8\n",
            "SS 2000 07+12 70.0 77.0\n",
            "78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4\n",
            "RPN\u0003300 07 68.5 74.1\n",
            "77.2 67.7 53.9 51.0 75.1 79.2 78.9 50.7 78.0 61.1 79.1 81.9 72.2 75.9 37.2 71.4 62.5 77.4 66.4\n",
            "RPN 300 07 69.9 70.0\n",
            "80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6\n",
            "RPN 300 07+12 73.2 76.5\n",
            "79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\n",
            "RPN 300 COCO+07+12 78.8 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9\n",
            "Table 7: Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time\n",
            "proposals for Fast R-CNN are 2000.\n",
            "method #\n",
            "box data mAP ar\n",
            "eo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\n",
            "SS 2000 12 65.7 80.3\n",
            "74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7\n",
            "SS 2000 07++12 68.4 82.3\n",
            "78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4\n",
            "64.2\n",
            "RPN 300 12 67.0 82.3\n",
            "76.4 71.0 48.4 45.2 72.1 72.3 87.3 42.2 73.7 50.0 86.8 78.7 78.4 77.4 34.5 70.1 57.1 77.1 58.9\n",
            "RPN 300 07++12 70.4 84.9\n",
            "79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\n",
            "RPN 300 COCO+07++12 75.9 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2\n",
            "  \n",
            "  \n",
            "  \n",
            "Figure 4: Recall vs. IoU overlap ratio on the PASCAL VOC 2007 test set.\n",
            "Table 11: One-Stage Detection vs. Two-Stage Proposal + Detection. Detection results are on the PASCAL\n",
            "VOC 2007 test set using the ZF model and Fast R-CNN. RPN uses unshared features.\n",
            "proposals detector mAP (%)\n",
            "Two-Stage RPN + ZF, unshared 300 Fast R-CNN + ZF, 1 scale 58.7\n",
            "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 1 scale 53.8\n",
            "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 5 scales 53.9\n",
            "Table 8: Detection results of Faster R-CNN on PAS-\n",
            "CAL VOC 2007 test set using different settings of\n",
            "anchors. The network is VGG-16. The training data\n",
            "is VOC 2007 trainval. The default setting of using 3\n",
            "scales and 3 aspect ratios (69.9%) is the same as that\n",
            "in Table 3.\n",
            "settings anchor scales aspect ratios mAP (%)\n",
            "1 scale, 1 ratio12821:1 65.8\n",
            "25621:1 66.7\n",
            "1 scale, 3 ratios1282f2:1, 1:1, 1:2g 68.8\n",
            "2562f2:1, 1:1, 1:2g 67.9\n",
            "3 scales, 1 ratios f1282;2562;5122g 1:1 69.8\n",
            "3 scales, 3 ratios f1282;2562;5122gf2:1, 1:1, 1:2g 69.9\n",
            "investigate the settings of anchors. By default we use 635\n",
            "3 scales and 3 aspect ratios (69.9% mAP in Table 8). 636\n",
            "If using just one anchor at each position, the mAP 637\n",
            "drops by a considerable margin of 3-4%. The mAP 638\n",
            "is higher if using 3 scales (with 1 aspect ratio) or 3 639\n",
            "aspect ratios (with 1 scale), demonstrating that using 640\n",
            "anchors of multiple sizes as the regression references 641\n",
            "is an effective solution. Using just 3 scales with 1 642Table 9: Detection results of Faster R-CNN on PAS-\n",
            "CAL VOC 2007 test set using different values of \u0015\n",
            "in Equation (1). The network is VGG-16. The training\n",
            "data is VOC 2007 trainval. The default setting of using\n",
            "\u0015= 10 (69.9%) is the same as that in Table 3.\n",
            "\u0015 0.1 1 10 100\n",
            "mAP (%) 67.2 68.9 69.9 69.1\n",
            "Table 10: Detection results of Faster R-CNN on PAS-\n",
            "CAL VOC 2007 test set using different numbers of\n",
            "proposals in testing. The network is VGG-16. The\n",
            "training data is VOC 2007 trainval. The default setting\n",
            "of using 300 proposals is the same as that in Table 3.\n",
            "# proposals 50 100 150 200 300 500 1000\n",
            "mAP (%) 66.3 68.9 69.5 69.8 69.9 69.8 69.8\n",
            "aspect ratio (69.8%) is as good as using 3 scales with 643\n",
            "3 aspect ratios on this dataset, suggesting that scales 644\n",
            "and aspect ratios are not disentangled dimensions for 645\n",
            "the detection accuracy. But we still adopt these two 646\n",
            "dimensions in our designs to keep our system ﬂexible. 647\n",
            "In Table 9 we compare different values of \u0015in Equa- 648\n",
            "tion (1). By default we use \u0015= 10 which makes the 649\n",
            "Page No.:10\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "10\n",
            "two terms in Equation (1) roughly equally weighted 650\n",
            "after normalization. Table 9 shows that our result is 651\n",
            "impacted just marginally (by \u00181%) when\u0015is within 652\n",
            "a scale of about two orders of magnitude (1 to 100). 653\n",
            "This demonstrates that the result is insensitive to \u0015in 654\n",
            "a wide range. 655\n",
            "In Table 10 we investigate the numbers of proposals 656\n",
            "in testing. 657\n",
            "Analysis of Recall-to-IoU. Next we compute the 658\n",
            "recall of proposals at different IoU ratios with ground- 659\n",
            "truth boxes. It is noteworthy that the Recall-to-IoU 660\n",
            "metric is just loosely [19], [20], [21] related to the 661\n",
            "ultimate detection accuracy. It is more appropriate to 662\n",
            "use this metric to diagnose the proposal method than 663\n",
            "to evaluate it. 664\n",
            "In Figure 4, we show the results of using 300, 1000, 665\n",
            "and 2000 proposals. We compare with SS, EB and 666\n",
            "MCG, and the Nproposals are the top-N ranked ones 667\n",
            "based on the conﬁdence generated by these meth- 668\n",
            "ods. The plots show that the RPN method behaves 669\n",
            "gracefully when the number of proposals drops from 670\n",
            "2000 to 300. This explains why the RPN has a good 671\n",
            "ultimate detection mAP when using as few as 300 672\n",
            "proposals. As we analyzed before, this property is 673\n",
            "mainly attributed to the clsterm of the RPN. The recall 674\n",
            "of SS, EB and MCG drops more quickly than RPN 675\n",
            "when the proposals are fewer. 676\n",
            "One-Stage Detection vs. Two-Stage Proposal + De- 677\n",
            "tection. The OverFeat paper [9] proposes a detection 678\n",
            "method that uses regressors and classiﬁers on sliding 679\n",
            "windows over convolutional feature maps. OverFeat 680\n",
            "is aone-stage, class-speciﬁc detection pipeline, and ours 681\n",
            "is a two-stage cascade consisting of class-agnostic pro- 682\n",
            "posals and class-speciﬁc detections. In OverFeat, the 683\n",
            "region-wise features come from a sliding window of 684\n",
            "one aspect ratio over a scale pyramid. These features 685\n",
            "are used to simultaneously determine the location and 686\n",
            "category of objects. In RPN, the features are from 687\n",
            "square (3\u00023) sliding windows and predict proposals 688\n",
            "relative to anchors with different scales and aspect 689\n",
            "ratios. Though both methods use sliding windows, the 690\n",
            "region proposal task is only the ﬁrst stage of Faster R- 691\n",
            "CNN—the downstream Fast R-CNN detector attends 692\n",
            "to the proposals to reﬁne them. In the second stage of 693\n",
            "our cascade, the region-wise features are adaptively 694\n",
            "pooled [1], [2] from proposal boxes that more faith- 695\n",
            "fully cover the features of the regions. We believe 696\n",
            "these features lead to more accurate detections. 697\n",
            "To compare the one-stage and two-stage systems, 698\n",
            "weemulate the OverFeat system (and thus also circum- 699\n",
            "vent other differences of implementation details) by 700\n",
            "one-stage Fast R-CNN. In this system, the “proposals” 701\n",
            "are dense sliding windows of 3 scales (128, 256, 512) 702\n",
            "and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is 703\n",
            "trained to predict class-speciﬁc scores and regress box 704\n",
            "locations from these sliding windows. Because the 705\n",
            "OverFeat system adopts an image pyramid, we also 706evaluate using convolutional features extracted from 707\n",
            "5 scales. We use those 5 scales as in [1], [2]. 708\n",
            "Table 11 compares the two-stage system and two 709\n",
            "variants of the one-stage system. Using the ZF model, 710\n",
            "the one-stage system has an mAP of 53.9%. This is 711\n",
            "lower than the two-stage system (58.7%) by 4.8%. 712\n",
            "This experiment justiﬁes the effectiveness of cascaded 713\n",
            "region proposals and object detection. Similar obser- 714\n",
            "vations are reported in [2], [39], where replacing SS 715\n",
            "region proposals with sliding windows leads to \u00186% 716\n",
            "degradation in both papers. We also note that the one- 717\n",
            "stage system is slower as it has considerably more 718\n",
            "proposals to process. 719\n",
            "4.2 Experiments on MS COCO 720\n",
            "We present more results on the Microsoft COCO 721\n",
            "object detection dataset [12]. This dataset involves 80 722\n",
            "object categories. We experiment with the 80k images 723\n",
            "on the training set, 40k images on the validation set, 724\n",
            "and 20k images on the test-dev set. We evaluate the 725\n",
            "mAP averaged for IoU 2[0:5 : 0:05 : 0:95] (COCO’s 726\n",
            "standard metric, simply denoted as mAP@[.5, .95]) 727\n",
            "and mAP@0.5 (PASCAL VOC’s metric). 728\n",
            "There are a few minor changes of our system made 729\n",
            "for this dataset. We train our models on an 8-GPU 730\n",
            "implementation, and the effective mini-batch size be- 731\n",
            "comes 8 for RPN (1 per GPU) and 16 for Fast R-CNN 732\n",
            "(2 per GPU). The RPN step and Fast R-CNN step are 733\n",
            "both trained for 240k iterations with a learning rate 734\n",
            "of 0.003 and then for 80k iterations with 0.0003. We 735\n",
            "modify the learning rates (starting with 0.003 instead 736\n",
            "of 0.001) because the mini-batch size is changed. For 737\n",
            "the anchors, we use 3 aspect ratios and 4 scales 738\n",
            "(adding 642), mainly motivated by handling small 739\n",
            "objects on this dataset. In addition, in our Fast R-CNN 740\n",
            "step, the negative samples are deﬁned as those with 741\n",
            "a maximum IoU with ground truth in the interval of 742\n",
            "[0;0:5), instead of [0:1;0:5) used in [1], [2]. We note 743\n",
            "that in the SPPnet system [1], the negative samples 744\n",
            "in[0:1;0:5) are used for network ﬁne-tuning, but the 745\n",
            "negative samples in [0;0:5) are still visited in the SVM 746\n",
            "step with hard-negative mining. But the Fast R-CNN 747\n",
            "system [2] abandons the SVM step, so the negative 748\n",
            "samples in [0;0:1) are never visited. Including these 749\n",
            "[0;0:1) samples improves mAP@0.5 on the COCO 750\n",
            "dataset for both Fast R-CNN and Faster R-CNN sys- 751\n",
            "tems (but the impact is negligible on PASCAL VOC). 752\n",
            "The rest of the implementation details are the same 753\n",
            "as on PASCAL VOC. In particular, we keep using 754\n",
            "300 proposals and single-scale (s = 600) testing. The 755\n",
            "testing time is still about 200ms per image on the 756\n",
            "COCO dataset. 757\n",
            "In Table 12 we ﬁrst report the results of the Fast 758\n",
            "R-CNN system [2] using the implementation in this 759\n",
            "paper. Our Fast R-CNN baseline has 39.3% mAP@0.5 760\n",
            "on the test-dev set, higher than that reported in [2]. 761\n",
            "We conjecture that the reason for this gap is mainly 762\n",
            "Page No.:11\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "11\n",
            "Table 12: Object detection results (%) on the MS COCO dataset. The model is VGG-16.\n",
            "COCO val COCO test-dev\n",
            "method proposals training data mAP@.5 mAP@[.5, .95] mAP@.5 mAP@[.5, .95]\n",
            "Fast R-CNN [2] SS, 2000 COCO train - - 35.9 19.7\n",
            "Fast R-CNN [impl. in this paper] SS, 2000 COCO train 38.6 18.9 39.3 19.3\n",
            "Faster R-CNN RPN, 300 COCO train 41.5 21.2 42.1 21.5\n",
            "Faster R-CNN RPN, 300 COCO trainval - - 42.7 21.9\n",
            "due to the deﬁnition of the negative samples and also 763\n",
            "the changes of the mini-batch sizes. We also note that 764\n",
            "the mAP@[.5, .95] is just comparable. 765\n",
            "Next we evaluate our Faster R-CNN system. Using 766\n",
            "the COCO training set to train, Faster R-CNN has 767\n",
            "42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the 768\n",
            "COCO test-dev set. This is 2.8% higher for mAP@0.5 769\n",
            "and2.2% higher for mAP@[.5, .95] than the Fast R- 770\n",
            "CNN counterpart under the same protocol (Table 12). 771\n",
            "This indicates that RPN performs excellent for im- 772\n",
            "proving the localization accuracy at higher IoU thresh- 773\n",
            "olds. Using the COCO trainval set to train, Faster R- 774\n",
            "CNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on 775\n",
            "the COCO test-dev set. Figure 6 shows some results 776\n",
            "on the MS COCO test-dev set. 777\n",
            "Faster R-CNN in ILSVRC & COCO 2015 compe- 778\n",
            "titions We have demonstrated that Faster R-CNN 779\n",
            "beneﬁts more from better features, thanks to the fact 780\n",
            "that the RPN completely learns to propose regions by 781\n",
            "neural networks. This observation is still valid even 782\n",
            "when one increases the depth substantially to over 783\n",
            "100 layers [18]. Only by replacing VGG-16 with a 101- 784\n",
            "layer residual net (ResNet-101) [18], the Faster R-CNN 785\n",
            "system increases the mAP from 41.5%/21.2% (VGG- 786\n",
            "16) to 48.4%/27.2% (ResNet-101) on the COCO val 787\n",
            "set. With other improvements orthogonal to Faster R- 788\n",
            "CNN, He et al. [18] obtained a single-model result of 789\n",
            "55.7%/34.9% and an ensemble result of 59.0%/37.4% 790\n",
            "on the COCO test-dev set, which won the 1st place 791\n",
            "in the COCO 2015 object detection competition. The 792\n",
            "same system [18] also won the 1st place in the ILSVRC 793\n",
            "2015 object detection competition, surpassing the sec- 794\n",
            "ond place by absolute 8.5%. RPN is also a building 795\n",
            "block of the 1st-place winning entries in ILSVRC 2015 796\n",
            "localization and COCO 2015 segmentation competi- 797\n",
            "tions, for which the details are available in [18] and 798\n",
            "[15] respectively. 799\n",
            "4.3 From MS COCO to PASCAL VOC 800\n",
            "Large-scale data is of crucial importance for improv- 801\n",
            "ing deep neural networks. Next, we investigate how 802\n",
            "the MS COCO dataset can help with the detection 803\n",
            "performance on PASCAL VOC. 804\n",
            "As a simple baseline, we directly evaluate the 805\n",
            "COCO detection model on the PASCAL VOC dataset, 806\n",
            "without ﬁne-tuning on any P ASCAL VOC data. This 807\n",
            "evaluation is possible because the categories on 808\n",
            "COCO are a superset of those on PASCAL VOC. The 809\n",
            "categories that are exclusive on COCO are ignored in 810Table 13: Detection mAP (%) of Faster R-CNN on\n",
            "PASCAL VOC 2007 test set and 2012 test set us-\n",
            "ing different training data. The model is VGG-16.\n",
            "“COCO” denotes that the COCO trainval set is used\n",
            "for training. See also Table 6 and Table 7.\n",
            "training data 2007 test 2012 test\n",
            "VOC07 69.9 67.0\n",
            "VOC07+12 73.2 -\n",
            "VOC07++12 - 70.4\n",
            "COCO (no VOC) 76.1 73.0\n",
            "COCO+VOC07+12 78.8 -\n",
            "COCO+VOC07++12 - 75.9\n",
            "Cor: 77.1%Loc: 8.1%VOC07+12\n",
            "  \n",
            "Cor: 77.1%\n",
            "Loc: 8.1%\n",
            "Sim: 2.0%\n",
            "Oth: 1.3%\n",
            "BG: 11.6%\n",
            "Cor: 83.3%Loc: 7.1%COCO+VOC07+12\n",
            "  \n",
            "Cor: 83.3%\n",
            "Loc: 7.1%\n",
            "Sim: 1.7%\n",
            "Oth: 1.3%\n",
            "BG: 6.7%\n",
            "Figur\n",
            "e 7: Error analyses on models trained with and\n",
            "without MS COCO data. The test set is PASCAL VOC\n",
            "2007 test. Distribution of top-ranked Cor (correct), Loc\n",
            "(false due to poor localization), Sim (confusion with\n",
            "a similar category), Oth (confusion with a dissimlar\n",
            "category), BG (ﬁred on background) is shown, which\n",
            "is generated by the published diagnosis code of [40].\n",
            "this experiment, and the softmax layer is performed 811\n",
            "only on the 20 categories plus background. The mAP 812\n",
            "under this setting is 76.1% on the PASCAL VOC 2007 813\n",
            "test set (Table 13). This result is better than that trained 814\n",
            "on VOC07+12 (73.2%) by a good margin, even though 815\n",
            "the PASCAL VOC data are not exploited. 816\n",
            "Then we ﬁne-tune the COCO detection model on 817\n",
            "the VOC dataset. In this experiment, the COCO model 818\n",
            "is in place of the ImageNet-pre-trained model (that 819\n",
            "is used to initialize the network weights), and the 820\n",
            "Faster R-CNN system is ﬁne-tuned as described in 821\n",
            "Section 3.2. Doing so leads to 78.8% mAP on the 822\n",
            "PASCAL VOC 2007 test set. The extra data from 823\n",
            "the COCO set increases the mAP by 5.6%. Table 6 824\n",
            "shows that the model trained on COCO+VOC has 825\n",
            "the best AP for every individual category on PASCAL 826\n",
            "VOC 2007. This improvement is mainly resulted from 827\n",
            "fewer false alarms on background (Figure 7). Similar 828\n",
            "improvements are observed on the PASCAL VOC 829\n",
            "2012 test set (Table 13 and Table 7). We note that 830\n",
            "Page No.:12\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "12\n",
            "bottle : 0.726\n",
            "person : 0.992\n",
            "dog : 0.981\n",
            "bicycle : 0.987\n",
            "bicycle : 0.977\n",
            " 7\n",
            "7\n",
            "7bicycle : 0.972\n",
            "person : 0.995\n",
            "person : 0.994\n",
            " e : 0.987e:09 8 7\n",
            " bicycbicyc\n",
            "bicycb\n",
            "4\n",
            "44person : 0.981\n",
            "person : 0.975\n",
            "person : 0.972\n",
            "person : 0.948\n",
            "person : 0.919\n",
            "horse : 0.984\n",
            "person : 0.670\n",
            "bird : 0.997\n",
            "bird : 0.727\n",
            "car : 1.000\n",
            "car : 0.982\n",
            "car car car : 0.981\n",
            "car : 0.880\n",
            "bottle : 0.826\n",
            "chair : 0.630\n",
            "diningtable : 0.862\n",
            "pottedplant : 0.728\n",
            "boat : 0.995\n",
            "boat : 0.948\n",
            "boat : 0.808\n",
            " : 0 808:boat : 0.692\n",
            "boat : 0.992\n",
            "boat : 0.846\n",
            "boat : 0.693\n",
            "bottle : 0.962\n",
            " 0 962bottle : 0.851\n",
            "diningtable : 0.791\n",
            "person : 0.962\n",
            "person : 0.930\n",
            "pottedplant : 0.951\n",
            "dog : 0.987\n",
            "person : 0.940\n",
            " 940940person : 0.893\n",
            "cat : 0.998\n",
            "car : 1.000\n",
            "person : 0.917\n",
            "boat : 0.895\n",
            "boat : 0.877\n",
            "boat : 0.749\n",
            "boat : 0.671\n",
            "person : 0.988\n",
            "car : 0.955\n",
            " 55\n",
            "55car : 0.745\n",
            " .745\n",
            "horse : 0.991\n",
            "person : 0.988\n",
            "person : 0.797\n",
            " bird : 0.978\n",
            "bird : 0.972\n",
            "bird : 0.941\n",
            "bird : 0.902\n",
            "person : 0.918\n",
            "cow : 0.998\n",
            "cow : 0.995\n",
            "aeroplane : 0.992\n",
            "aeroplane : 0.986\n",
            "sheep : 0.970\n",
            "bird : 0.998\n",
            "bird : 0.980\n",
            "bird : 0.806\n",
            "pottedplant : 0.993\n",
            "pottedplant : 0.940\n",
            "pottedplant : 0.869\n",
            "pottedplant : 0.820\n",
            "pottedplant : 0.715\n",
            "aeroplane : 0.998\n",
            "car : 0.907\n",
            " 907907person : 0.993\n",
            " person : 0.987\n",
            "chair : 0.984\n",
            "chair : 0.978\n",
            "chair : 0.976\n",
            "chair : 0.962\n",
            "984984diningtable : 0.997\n",
            "bottle : 0.789\n",
            "chair : 0.723\n",
            "diningtable : 0.903\n",
            "e:0.789\n",
            " person : 0.968\n",
            "tvmonitor : 0.993\n",
            "tvmonitor : 0.945\n",
            "aeroplane : 0.978\n",
            "person : 0.988\n",
            "bottle : 0.903\n",
            "bottle : 0.884\n",
            "bottle : 0.858\n",
            "bb\n",
            "bottle : 0 bot\n",
            "bottle : 0.616\n",
            "chair : 0.982\n",
            "chair : 0.852\n",
            "person : 0.983\n",
            "person : 0.959\n",
            ": 0 903person : 0.897\n",
            "person : 0.870\n",
            "tvmonitor : 0.993\n",
            "dog : 0.697\n",
            "person : 0.961\n",
            "person : 0.960\n",
            "personperson\n",
            "person : 0.958\n",
            "person : 0.757\n",
            "bus : 0.999\n",
            "person : 0.996\n",
            "per\n",
            "perper person : 0.995\n",
            "person : 0.994\n",
            "person : 0.985\n",
            "cow : 0.985\n",
            "cow : 0.979\n",
            "cow : 0.979\n",
            "cow : 0.974\n",
            "cow : 0.892\n",
            "person : 0.998\n",
            "car : 0.999\n",
            "person : 0.929\n",
            " person : 0.994\n",
            "person : 0.991\n",
            "person : 0.988\n",
            "persp\n",
            "person : 0.976\n",
            "person : 0.964\n",
            "car : 0.997\n",
            "car : 0.980\n",
            "person : 0.993\n",
            "personperson\n",
            "personpersonperson : 0.986\n",
            "0 993\n",
            ":n\n",
            "86\n",
            "n:\n",
            "n:nperson : 0.959\n",
            "Figure 5: Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster\n",
            "R-CNN system. The model is VGG-16 and the training data is 07+12 trainval (73.2% mAP on the 2007 test\n",
            "set). Our method detects objects of a wide range of scales and aspect ratios. Each output box is associated\n",
            "with a category label and a softmax score in [0;1]. A score threshold of 0.6 is used to display these images.\n",
            "The running time for obtaining these results is 198ms per image, including all steps.\n",
            "the test-time speed of obtaining these state-of-the-art 831\n",
            "results is still about 200ms per image. 832\n",
            "5 C ONCLUSION 833\n",
            "We have presented RPNs for efﬁcient and accurate 834\n",
            "region proposal generation. By sharing convolutional 835\n",
            "features with the down-stream detection network, the 836\n",
            "region proposal step is nearly cost-free. Our method 837\n",
            "enables a uniﬁed, deep-learning-based object detec- 838\n",
            "tion system to run at 5-17 fps. The learned RPN also 839improves region proposal quality and thus the overall 840\n",
            "object detection accuracy. 841\n",
            "REFERENCES 842\n",
            "[1] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling 843\n",
            "in deep convolutional networks for visual recognition,” in 844\n",
            "European Conference on Computer Vision (ECCV), 2014. 845\n",
            "[2] R. Girshick, “Fast R-CNN,” in IEEE International Conference on 846\n",
            "Computer Vision (ICCV), 2015. 847\n",
            "[3] K. Simonyan and A. Zisserman, “Very deep convolutional 848\n",
            "networks for large-scale image recognition,” in International 849\n",
            "Conference on Learning Representations (ICLR), 2015. 850\n",
            "Page No.:13\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "13\n",
            "cup : 0.807\n",
            "bowl : 0.847\n",
            "bowl : 0.816\n",
            "bowl : 0.744\n",
            "bowl : 0.710\n",
            "chair : 0.772\n",
            "dining table : 0.618\n",
            "oven : 0.969\n",
            "refrigerator : 0.631\n",
            "cup : 0.990\n",
            "pizza : 0.919\n",
            "dining table : 0.888\n",
            "person : 0.984\n",
            "personpersoncar : 0.816\n",
            "pizza : 0.965\n",
            "clock : 0.988\n",
            "person : 0.998\n",
            "kite : 0.934\n",
            "toothbrush : 0.668\n",
            "teddy bear : 0.999\n",
            "teddy bear : 0.890\n",
            "teddy bear : 0.802\n",
            " teddy bear : 0.738\n",
            "bowl : 0.602\n",
            "potted plant : 0.769\n",
            "toilet : 0.921\n",
            "sink : 0.969\n",
            "sink : 0.994\n",
            "sink : 0.992\n",
            "sink : 0.976\n",
            " 66sink : 0.938\n",
            "person : 0.970\n",
            " : 0.970\n",
            " ersonperson : 0.869\n",
            "bus : 0.999\n",
            "bottle : 0.768\n",
            "cup : 0.720\n",
            "chair : 0.644\n",
            "tv : 0.964\n",
            "tv : 0.959\n",
            "laptop : 0.986\n",
            "mouse : 0.871\n",
            "mouse : 0.677\n",
            "m\n",
            "keyboard : 0.956\n",
            "book : 0.611\n",
            "person : 0.986\n",
            "boat : 0.758\n",
            "boat : 0.746\n",
            " boat : 0.613\n",
            "bench : 0.971\n",
            "train : 0.965\n",
            "traffic light : 0.869\n",
            "traffic light : 0.713\n",
            "chair : 0.631\n",
            "couch : 0.991\n",
            "couch : 0.719\n",
            "couch : 0.627\n",
            "dining table : 0.637\n",
            "dog : 0.966\n",
            "frisbee : 0.998\n",
            "bird : 0.987\n",
            "bird : 0.968\n",
            "bird : 0.894\n",
            "person : 0.723\n",
            "cup : 0.986\n",
            "cup : 0.931\n",
            "bowl : 0.958\n",
            "sandwich : 0.629\n",
            "dining table : 0.941\n",
            "zebra : 0.996\n",
            "zebra : 0.993\n",
            "zebra : 0.970\n",
            " 970zebra : 0.848\n",
            "person : 0.917\n",
            " person : 0.792\n",
            " : 0.7920 792\n",
            "tv : 0.711\n",
            "laptop : 0.973\n",
            "mouse : 0.981\n",
            "keyboard : 0.638\n",
            "keyboard : 0.615\n",
            "person : 0.999\n",
            "person : 0.999person : 0.999\n",
            "persopersotennis racket : 0.960\n",
            "bird : 0.956\n",
            "bird : 0.906\n",
            "bird : 0.746\n",
            "horse : 0.990\n",
            "person : 0.993\n",
            "bottle : 0.982\n",
            "oven : 0.655\n",
            "refrigerator : 0.699\n",
            "clock : 0.982\n",
            "bed : 0.999\n",
            "person : 0.808\n",
            "bottle : 0.627\n",
            "pizza : 0.995\n",
            "pizza : 0.985\n",
            "pizza : 0.982\n",
            "pizza : 0.938\n",
            "dining table : 0.956\n",
            "person : 0.998\n",
            "skis : 0.919\n",
            "bowl : 0.759\n",
            "broccoli : 0.953\n",
            "person : 0.999\n",
            "person : 0.934\n",
            "surfboard : 0.979\n",
            "person : 0.940\n",
            "person : 0.927\n",
            "person : 0.864\n",
            "0.940\n",
            "person : 0.854\n",
            "person : 0.825\n",
            " 5\n",
            "5person : 0.813\n",
            "person : 0.716\n",
            "person : 0.692\n",
            "p\n",
            "pperson : 0.691\n",
            "927\n",
            "927person : 0.665\n",
            "person : 0.618\n",
            "boat : 0.992\n",
            "umbrella : 0.885\n",
            "giraffe : 0.993\n",
            "giraffe : 0.989\n",
            "giraffe : 0.988\n",
            "person : 0.867\n",
            "airplane : 0.997\n",
            "person : 0.970\n",
            "person : 0.950\n",
            "person : 0.931\n",
            "p\n",
            "person : 0.916\n",
            "person : 0.897\n",
            "person : 0.842\n",
            " person : 0.841\n",
            " person : 0.84person : 0.772\n",
            "bicycle : 0.891\n",
            "bicycle : 0.639\n",
            "car : 0.957\n",
            "motorcycle : 0.827\n",
            "motorcycle : 0.713\n",
            "traffic light : 0.802\n",
            "umbrella : 0.824\n",
            "person : 0.800\n",
            "clock : 0.986\n",
            "clock : 0.981\n",
            "person : 0.996\n",
            "person : 0.976\n",
            "person : 0.975\n",
            "rson : 0.9 75 rson\n",
            "son : 0onperson : 0.958\n",
            "person : 0.950\n",
            "person : 0.941\n",
            "0.976\n",
            "0976person : 0.939\n",
            "pepe\n",
            "person : 0.928\n",
            " 958\n",
            "95\n",
            "8\n",
            "0975\n",
            "n : 0 .\n",
            " n:0\n",
            ".\n",
            "0.975\n",
            "0.9750.person : 0.823\n",
            "on : 0.950 050\n",
            "person : 0.805\n",
            "person : 0.766\n",
            "person : 0.759\n",
            ".9414\n",
            "person : 0.673\n",
            "dog : 0.996\n",
            "dog : 0.691\n",
            "0 939\n",
            "p\n",
            "backpack : 0.756\n",
            "handbag : 0.848\n",
            "Figure 6: Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN\n",
            "system. The model is VGG-16 and the training data is COCO trainval (42.7% mAP@0.5 on the test-dev set).\n",
            "Each output box is associated with a category label and a softmax score in [0;1]. A score threshold of 0.6 is\n",
            "used to display these images. For each image, one color represents one object category in that image.\n",
            "[4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeul- 851\n",
            "ders, “Selective search for object recognition,” International 852\n",
            "Journal of Computer Vision (IJCV), 2013. 853\n",
            "[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature 854\n",
            "hierarchies for accurate object detection and semantic seg- 855\n",
            "mentation,” in IEEE Conference on Computer Vision and Pattern 856\n",
            "Recognition (CVPR), 2014. 857\n",
            "[6] C. L. Zitnick and P . Doll ´ar, “Edge boxes: Locating object 858\n",
            "proposals from edges,” in European Conference on Computer 859\n",
            "Vision (ECCV), 2014. 860\n",
            "[7] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional 861\n",
            "networks for semantic segmentation,” in IEEE Conference on 862\n",
            "Computer Vision and Pattern Recognition (CVPR), 2015. 863\n",
            "[8] P . F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra- 864\n",
            "manan, “Object detection with discriminatively trained part- 865based models,” IEEE Transactions on Pattern Analysis and Ma- 866\n",
            "chine Intelligence (TP AMI), 2010. 867\n",
            "[9] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, 868\n",
            "and Y. LeCun, “Overfeat: Integrated recognition, localization 869\n",
            "and detection using convolutional networks,” in International 870\n",
            "Conference on Learning Representations (ICLR), 2014. 871\n",
            "[10] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards 872\n",
            "real-time object detection with region proposal networks,” in 873\n",
            "Neural Information Processing Systems (NIPS), 2015. 874\n",
            "[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and 875\n",
            "A. Zisserman, “The PASCAL Visual Object Classes Challenge 876\n",
            "2007 (VOC2007) Results,” 2007. 877\n",
            "[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ra- 878\n",
            "manan, P . Doll ´ar, and C. L. Zitnick, “Microsoft COCO: Com- 879\n",
            "mon Objects in Context,” in European Conference on Computer 880\n",
            "Page No.:14\n",
            "0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "14\n",
            "Vision (ECCV), 2014. 881\n",
            "[13] S. Song and J. Xiao, “Deep sliding shapes for amodal 3d object 882\n",
            "detection in rgb-d images,” arXiv:1511.02300, 2015. 883\n",
            "[14] J. Zhu, X. Chen, and A. L. Yuille, “DeePM: A deep part-based 884\n",
            "model for object detection and semantic part localization,” 885\n",
            "arXiv:1511.07131, 2015. 886\n",
            "[15] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmenta- 887\n",
            "tion via multi-task network cascades,” arXiv:1512.04412, 2015. 888\n",
            "[16] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully 889\n",
            "convolutional localization networks for dense captioning,” 890\n",
            "arXiv:1511.07571, 2015. 891\n",
            "[17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, “Human cu- 892\n",
            "ration and convnets: Powering item-to-item recommendations 893\n",
            "on pinterest,” arXiv:1511.04003, 2015. 894\n",
            "[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning 895\n",
            "for image recognition,” arXiv:1512.03385, 2015. 896\n",
            "[19] J. Hosang, R. Benenson, and B. Schiele, “How good are de- 897\n",
            "tection proposals, really?” in British Machine Vision Conference 898\n",
            "(BMVC), 2014. 899\n",
            "[20] J. Hosang, R. Benenson, P . Doll ´ar, and B. Schiele, “What makes 900\n",
            "for effective detection proposals?” IEEE Transactions on Pattern 901\n",
            "Analysis and Machine Intelligence (TP AMI), 2015. 902\n",
            "[21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra, 903\n",
            "“Object-Proposal Evaluation Protocol is ’Gameable’,” arXiv: 904\n",
            "1505.05836, 2015. 905\n",
            "[22] J. Carreira and C. Sminchisescu, “CPMC: Automatic ob- 906\n",
            "ject segmentation using constrained parametric min-cuts,” 907\n",
            "IEEE Transactions on Pattern Analysis and Machine Intelligence 908\n",
            "(TP AMI), 2012. 909\n",
            "[23] P . Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, 910\n",
            "“Multiscale combinatorial grouping,” in IEEE Conference on 911\n",
            "Computer Vision and Pattern Recognition (CVPR), 2014. 912\n",
            "[24] B. Alexe, T. Deselaers, and V . Ferrari, “Measuring the object- 913\n",
            "ness of image windows,” IEEE Transactions on Pattern Analysis 914\n",
            "and Machine Intelligence (TP AMI), 2012. 915\n",
            "[25] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks 916\n",
            "for object detection,” in Neural Information Processing Systems 917\n",
            "(NIPS), 2013. 918\n",
            "[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable 919\n",
            "object detection using deep neural networks,” in IEEE Confer- 920\n",
            "ence on Computer Vision and Pattern Recognition (CVPR), 2014. 921\n",
            "[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, “Scalable, 922\n",
            "high-quality object detection,” arXiv:1412.1441 (v1), 2015. 923\n",
            "[28] P . O. Pinheiro, R. Collobert, and P . Dollar, “Learning to 924\n",
            "segment object candidates,” in Neural Information Processing 925\n",
            "Systems (NIPS), 2015. 926\n",
            "[29] J. Dai, K. He, and J. Sun, “Convolutional feature masking 927\n",
            "for joint object and stuff segmentation,” in IEEE Conference on 928\n",
            "Computer Vision and Pattern Recognition (CVPR), 2015. 929\n",
            "[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Ob- 930\n",
            "ject detection networks on convolutional feature maps,” 931\n",
            "arXiv:1504.06066, 2015. 932\n",
            "[31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and 933\n",
            "Y. Bengio, “Attention-based models for speech recognition,” 934\n",
            "inNeural Information Processing Systems (NIPS), 2015. 935\n",
            "[32] M. D. Zeiler and R. Fergus, “Visualizing and understanding 936\n",
            "convolutional neural networks,” in European Conference on 937\n",
            "Computer Vision (ECCV), 2014. 938\n",
            "[33] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve 939\n",
            "restricted boltzmann machines,” in International Conference on 940\n",
            "Machine Learning (ICML), 2010. 941\n",
            "[34] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov, 942\n",
            "D. Erhan, and A. Rabinovich, “Going deeper with convo- 943\n",
            "lutions,” in IEEE Conference on Computer Vision and Pattern 944\n",
            "Recognition (CVPR), 2015. 945\n",
            "[35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, 946\n",
            "W. Hubbard, and L. D. Jackel, “Backpropagation applied to 947\n",
            "handwritten zip code recognition,” Neural computation, 1989. 948\n",
            "[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, 949\n",
            "Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 950\n",
            "and L. Fei-Fei, “ImageNet Large Scale Visual Recognition 951\n",
            "Challenge,” in International Journal of Computer Vision (IJCV) , 952\n",
            "2015. 953\n",
            "[37] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classi- 954\n",
            "ﬁcation with deep convolutional neural networks,” in Neural 955\n",
            "Information Processing Systems (NIPS), 2012. 956[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- 957\n",
            "shick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional 958\n",
            "architecture for fast feature embedding,” arXiv:1408.5093, 2014. 959\n",
            "[39] K. Lenc and A. Vedaldi, “R-CNN minus R,” in British Machine 960\n",
            "Vision Conference (BMVC), 2015. 961\n",
            "[40] D. Hoiem, Y. Chodpathumwan, and Q. Dai, “Diagnosing error 962\n",
            "in object detectors,” in European Conference on Computer Vision 963\n",
            "(ECCV), 2012. 964\n",
            "Shaoqing Ren received the BS degree from 965\n",
            "the University of Science and Technology of 966\n",
            "China in 2011. He is currently a PhD student 967\n",
            "in a joint PhD program between University 968\n",
            "of Science and Technology of China and 969\n",
            "Microsoft Research Asia. His research in- 970\n",
            "terests are in computer vision, especially in 971\n",
            "detection and localization of general objects 972\n",
            "and faces. 973\n",
            "974\n",
            "Kaiming He is a lead researcher at Microsoft 975\n",
            "Research Asia. He received the BS degree 976\n",
            "from Tsinghua University in 2007, and the 977\n",
            "PhD degree from the Chinese University of 978\n",
            "Hong Kong in 2011. He joined Microsoft Re- 979\n",
            "search Asia in 2011. His current research 980\n",
            "interests are deep learning for visual recog- 981\n",
            "nition, including image classiﬁcation, object 982\n",
            "detection, and semantic segmentation. He 983\n",
            "has won the Best Paper Award at CVPR 984\n",
            "2009. 985\n",
            "Ross Girshick is a Research Scientist at 986\n",
            "Facebook AI Research. He holds a PhD and 987\n",
            "MS in computer science, both from the Uni- 988\n",
            "versity of Chicago where he studied under 989\n",
            "the supervision of Pedro Felzenszwalb. Prior 990\n",
            "to joining Facebook AI Research, Ross was 991\n",
            "a Researcher at Microsoft Research, and 992\n",
            "a Postdoctorial Fellow at the University of 993\n",
            "California, Berkeley where he collaborated 994\n",
            "with Jitendra Malik and Trevor Darrell. During 995\n",
            "the course of PASCAL VOC object detection 996\n",
            "challenge, Ross participated in multiple winning object detection 997\n",
            "entries and was awarded a “lifetime achievement” prize for his work 998\n",
            "on the widely used Deformable Part Models. 999\n",
            "Jian Sun is a principal researcher at Mi- 1000\n",
            "crosoft Research Asia. He got the BS de- 1001\n",
            "gree, MS degree and PhD degree from Xian 1002\n",
            "Jiaotong University in 1997, 2000 and 2003. 1003\n",
            "He joined Microsoft Research Asia in July, 1004\n",
            "2003. His current major research interests 1005\n",
            "are computer vision, computational photog- 1006\n",
            "raphy, and deep learning. He has won the 1007\n",
            "Best Paper Award at CVPR 2009. 1008\n",
            "1009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txtSplit = ''\n",
        "pageContent = ''\n",
        "txtSplit1 = ''\n",
        "for i in range(pdfReader.getNumPages()):\n",
        "  if i < 3:\n",
        "    if i == 0:\n",
        "      page = pdfReader.getPage(i)\n",
        "      p1 = str(1+pdfReader.getPageNumber(page))\n",
        "     #print(\"Page No.:\" + p1)\n",
        "      pageContent = page.extractText()\n",
        "      txtSplit =pageContent.split(\"F\",1)[1]\n",
        "#print(txtSplit)\n",
        "\n",
        "  elif i == 1:\n",
        "    page = pdfReader.getPage(i)\n",
        "    p1 = str(1+pdfReader.getPageNumber(page))\n",
        "  #print(\"Page No.:\" + p1)\n",
        "    pageContent = page.extractText()\n",
        "  #print(pageContent)\n",
        "  elif i == 2:\n",
        "        page = pdfReader.getPage(i)\n",
        "        p1 = str(1+pdfReader.getPageNumber(page))\n",
        "  #print(\"Page No.:\" + p1)\n",
        "        pageContent = page.extractText()\n",
        "        txtSplit1 =pageContent.split(\"2L ITERATURE REVIEW\",1)[0]\n",
        "  #print(txtSplit1)\n",
        "  text = txtSplit+pageContent+txtSplit1\n",
        "  print(text)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IWaZXnt2F2p",
        "outputId": "f6e29878-8d99-4879-dbfe-801d7d76e14a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n",
            "aster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n",
            "10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "1\n",
            "Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks\n",
            "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
            "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
            "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
            "’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection\n",
            "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
            "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
            "made publicly available.\n",
            "Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.\n",
            "F\n",
            "1 I NTRODUCTION 1\n",
            "Recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (R- 4\n",
            "CNNs) [5]. Although region-based CNNs were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. The latest 8\n",
            "incarnation, Fast R-CNN [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. Now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "Region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "Selective Search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. Yet when compared to efﬁcient 18\n",
            "detection networks [2], Selective Search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a CPU 20\n",
            "implementation. EdgeBoxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. Nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "\u000fS. Ren is with University of Science and Technology of China, Hefei,\n",
            "China. This work was done when S. Ren was an intern at Microsoft\n",
            "Research. Email: sqren@mail.ustc.edu.cn\n",
            "\u000fK. He and J. Sun are with Visual Computing Group, Microsoft\n",
            "Research. E-mail:fkahe,jiansung@microsoft.com\n",
            "\u000fR. Girshick is with Facebook AI Research. The majority of this work\n",
            "was done when R. Girshick was with Microsoft Research. E-mail:\n",
            "rbg@fb.comOne may note that fast region-based CNNs take 26\n",
            "advantage of GPUs, while the region proposal meth- 27\n",
            "ods used in research are implemented on the CPU, 28\n",
            "making such runtime comparisons inequitable. An ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the GPU. This may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "In this paper, we show that an algorithmic change— 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral network—leads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection network’s computation. To this end, we 39\n",
            "introduce novel Region Proposal Networks (RPNs) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. By sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "Our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like Fast R- 46\n",
            "CNN, can also be used for generating region pro- 47\n",
            "posals. On top of these convolutional features, we 48\n",
            "construct an RPN by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. The RPN is thus a kind of fully convo- 52\n",
            "lutional network (FCN) [7] and can be trained end-to- 53\n",
            "end speciﬁcally for the task for generating detection 54\n",
            "proposals. 55\n",
            "RPNs are designed to efﬁciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. In 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newText = text.encode('ascii ','ignore').lower()\n",
        "print(newText)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkBA3l-m2qOa",
        "outputId": "3512f7eb-885d-4656-c1ef-f559a1181ac3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'aster r-cnn: towards real-time object\\ndetection with region proposal networks\\nshaoqing ren, kaiming he, ross girshick, and jian sun\\nabstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\\nadvances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region\\nproposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image\\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional\\nnetwork that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to\\ngenerate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn\\ninto a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with\\nattention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3],\\nour detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection\\naccuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco\\n2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been\\nmade publicly available.\\nindex termsobject detection, region proposal, convolutional neural network.\\nf\\n1 i ntroduction 1\\nrecent advances in object detection are driven by 2\\nthe success of region proposal methods (e.g., [4]) 3\\nand region-based convolutional neural networks (r- 4\\ncnns) [5]. although region-based cnns were com- 5\\nputationally expensive as originally developed in [5], 6\\ntheir cost has been drastically reduced thanks to shar- 7\\ning convolutions across proposals [1], [2]. the latest 8\\nincarnation, fast r-cnn [2], achieves near real-time 9\\nrates using very deep networks [3], when ignoring the 10\\ntime spent on region proposals. now, proposals are the 11\\ntest-time computational bottleneck in state-of-the-art 12\\ndetection systems. 13\\nregion proposal methods typically rely on inex- 14\\npensive features and economical inference schemes. 15\\nselective search [4], one of the most popular meth- 16\\nods, greedily merges superpixels based on engineered 17\\nlow-level features. yet when compared to efcient 18\\ndetection networks [2], selective search is an order of 19\\nmagnitude slower, at 2 seconds per image in a cpu 20\\nimplementation. edgeboxes [6] currently provides the 21\\nbest tradeoff between proposal quality and speed, 22\\nat 0.2 seconds per image. nevertheless, the region 23\\nproposal step still consumes as much running time 24\\nas the detection network. 25\\n\\x0fs. ren is with university of science and technology of china, hefei,\\nchina. this work was done when s. ren was an intern at microsoft\\nresearch. email: sqren@mail.ustc.edu.cn\\n\\x0fk. he and j. sun are with visual computing group, microsoft\\nresearch. e-mail:fkahe,jiansung@microsoft.com\\n\\x0fr. girshick is with facebook ai research. the majority of this work\\nwas done when r. girshick was with microsoft research. e-mail:\\nrbg@fb.comone may note that fast region-based cnns take 26\\nadvantage of gpus, while the region proposal meth- 27\\nods used in research are implemented on the cpu, 28\\nmaking such runtime comparisons inequitable. an ob- 29\\nvious way to accelerate proposal computation is to re- 30\\nimplement it for the gpu. this may be an effective en- 31\\ngineering solution, but re-implementation ignores the 32\\ndown-stream detection network and therefore misses 33\\nimportant opportunities for sharing computation. 34\\nin this paper, we show that an algorithmic change 35\\ncomputing proposals with a deep convolutional neu- 36\\nral networkleads to an elegant and effective solution 37\\nwhere proposal computation is nearly cost-free given 38\\nthe detection networks computation. to this end, we 39\\nintroduce novel region proposal networks (rpns) that 40\\nshare convolutional layers with state-of-the-art object 41\\ndetection networks [1], [2]. by sharing convolutions at 42\\ntest-time, the marginal cost for computing proposals 43\\nis small (e.g., 10ms per image). 44\\nour observation is that the convolutional feature 45\\nmaps used by region-based detectors, like fast r- 46\\ncnn, can also be used for generating region pro- 47\\nposals. on top of these convolutional features, we 48\\nconstruct an rpn by adding a few additional con- 49\\nvolutional layers that simultaneously regress region 50\\nbounds and objectness scores at each location on a 51\\nregular grid. the rpn is thus a kind of fully convo- 52\\nlutional network (fcn) [7] and can be trained end-to- 53\\nend specically for the task for generating detection 54\\nproposals. 55\\nrpns are designed to efciently predict region pro- 56\\nposals with a wide range of scales and aspect ratios. in 57\\ncontrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission. see http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication. citation information: doi\\n10.1109/tpami.2016.2577031, ieee transactions on pattern analysis and machine intelligence\\n1\\nfaster r-cnn: towards real-time object\\ndetection with region proposal networks\\nshaoqing ren, kaiming he, ross girshick, and jian sun\\nabstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\\nadvances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region\\nproposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image\\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional\\nnetwork that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to\\ngenerate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn\\ninto a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with\\nattention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3],\\nour detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection\\naccuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco\\n2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been\\nmade publicly available.\\nindex termsobject detection, region proposal, convolutional neural network.\\nf\\n1 i ntroduction 1\\nrecent advances in object detection are driven by 2\\nthe success of region proposal methods (e.g., [4]) 3\\nand region-based convolutional neural networks (r- 4\\ncnns) [5]. although region-based cnns were com- 5\\nputationally expensive as originally developed in [5], 6\\ntheir cost has been drastically reduced thanks to shar- 7\\ning convolutions across proposals [1], [2]. the latest 8\\nincarnation, fast r-cnn [2], achieves near real-time 9\\nrates using very deep networks [3], when ignoring the 10\\ntime spent on region proposals. now, proposals are the 11\\ntest-time computational bottleneck in state-of-the-art 12\\ndetection systems. 13\\nregion proposal methods typically rely on inex- 14\\npensive features and economical inference schemes. 15\\nselective search [4], one of the most popular meth- 16\\nods, greedily merges superpixels based on engineered 17\\nlow-level features. yet when compared to efcient 18\\ndetection networks [2], selective search is an order of 19\\nmagnitude slower, at 2 seconds per image in a cpu 20\\nimplementation. edgeboxes [6] currently provides the 21\\nbest tradeoff between proposal quality and speed, 22\\nat 0.2 seconds per image. nevertheless, the region 23\\nproposal step still consumes as much running time 24\\nas the detection network. 25\\n\\x0fs. ren is with university of science and technology of china, hefei,\\nchina. this work was done when s. ren was an intern at microsoft\\nresearch. email: sqren@mail.ustc.edu.cn\\n\\x0fk. he and j. sun are with visual computing group, microsoft\\nresearch. e-mail:fkahe,jiansung@microsoft.com\\n\\x0fr. girshick is with facebook ai research. the majority of this work\\nwas done when r. girshick was with microsoft research. e-mail:\\nrbg@fb.comone may note that fast region-based cnns take 26\\nadvantage of gpus, while the region proposal meth- 27\\nods used in research are implemented on the cpu, 28\\nmaking such runtime comparisons inequitable. an ob- 29\\nvious way to accelerate proposal computation is to re- 30\\nimplement it for the gpu. this may be an effective en- 31\\ngineering solution, but re-implementation ignores the 32\\ndown-stream detection network and therefore misses 33\\nimportant opportunities for sharing computation. 34\\nin this paper, we show that an algorithmic change 35\\ncomputing proposals with a deep convolutional neu- 36\\nral networkleads to an elegant and effective solution 37\\nwhere proposal computation is nearly cost-free given 38\\nthe detection networks computation. to this end, we 39\\nintroduce novel region proposal networks (rpns) that 40\\nshare convolutional layers with state-of-the-art object 41\\ndetection networks [1], [2]. by sharing convolutions at 42\\ntest-time, the marginal cost for computing proposals 43\\nis small (e.g., 10ms per image). 44\\nour observation is that the convolutional feature 45\\nmaps used by region-based detectors, like fast r- 46\\ncnn, can also be used for generating region pro- 47\\nposals. on top of these convolutional features, we 48\\nconstruct an rpn by adding a few additional con- 49\\nvolutional layers that simultaneously regress region 50\\nbounds and objectness scores at each location on a 51\\nregular grid. the rpn is thus a kind of fully convo- 52\\nlutional network (fcn) [7] and can be trained end-to- 53\\nend specically for the task for generating detection 54\\nproposals. 55\\nrpns are designed to efciently predict region pro- 56\\nposals with a wide range of scales and aspect ratios. in 57\\ncontrast to prevalent methods [8], [9], [1], [2] that use 58'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Introduction From PDF"
      ],
      "metadata": {
        "id": "e_f98M1y6cBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install lxml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCQtstJK2y4a",
        "outputId": "0e71380a-ff69-4281-c274-0332b495a67c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.9.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "cleantext = BeautifulSoup(newText, \"lxml\")\n",
        "print(cleantext)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REzdVTXC20Mc",
        "outputId": "243f8c7c-ea7b-4ea6-8c8b-4867c3540265"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html><body><p>aster r-cnn: towards real-time object\n",
            "detection with region proposal networks\n",
            "shaoqing ren, kaiming he, ross girshick, and jian sun\n",
            "abstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "advances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn\n",
            "into a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with\n",
            "attention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection\n",
            "accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco\n",
            "2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been\n",
            "made publicly available.\n",
            "index termsobject detection, region proposal, convolutional neural network.\n",
            "f\n",
            "1 i ntroduction 1\n",
            "recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (r- 4\n",
            "cnns) [5]. although region-based cnns were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. the latest 8\n",
            "incarnation, fast r-cnn [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "selective search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. yet when compared to efcient 18\n",
            "detection networks [2], selective search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a cpu 20\n",
            "implementation. edgeboxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "s. ren is with university of science and technology of china, hefei,\n",
            "china. this work was done when s. ren was an intern at microsoft\n",
            "research. email: sqren@mail.ustc.edu.cn\n",
            "k. he and j. sun are with visual computing group, microsoft\n",
            "research. e-mail:fkahe,jiansung@microsoft.com\n",
            "r. girshick is with facebook ai research. the majority of this work\n",
            "was done when r. girshick was with microsoft research. e-mail:\n",
            "rbg@fb.comone may note that fast region-based cnns take 26\n",
            "advantage of gpus, while the region proposal meth- 27\n",
            "ods used in research are implemented on the cpu, 28\n",
            "making such runtime comparisons inequitable. an ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the gpu. this may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "in this paper, we show that an algorithmic change 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral networkleads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection networks computation. to this end, we 39\n",
            "introduce novel region proposal networks (rpns) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. by sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like fast r- 46\n",
            "cnn, can also be used for generating region pro- 47\n",
            "posals. on top of these convolutional features, we 48\n",
            "construct an rpn by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. the rpn is thus a kind of fully convo- 52\n",
            "lutional network (fcn) [7] and can be trained end-to- 53\n",
            "end specically for the task for generating detection 54\n",
            "proposals. 55\n",
            "rpns are designed to efciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. in 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission. see http://www.ieee.org/publications_standards/publications/rights/index.html for more\n",
            "information.this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication. citation information: doi\n",
            "10.1109/tpami.2016.2577031, ieee transactions on pattern analysis and machine intelligence\n",
            "1\n",
            "faster r-cnn: towards real-time object\n",
            "detection with region proposal networks\n",
            "shaoqing ren, kaiming he, ross girshick, and jian sun\n",
            "abstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
            "advances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region\n",
            "proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image\n",
            "convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional\n",
            "network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to\n",
            "generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn\n",
            "into a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with\n",
            "attention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3],\n",
            "our detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection\n",
            "accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco\n",
            "2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been\n",
            "made publicly available.\n",
            "index termsobject detection, region proposal, convolutional neural network.\n",
            "f\n",
            "1 i ntroduction 1\n",
            "recent advances in object detection are driven by 2\n",
            "the success of region proposal methods (e.g., [4]) 3\n",
            "and region-based convolutional neural networks (r- 4\n",
            "cnns) [5]. although region-based cnns were com- 5\n",
            "putationally expensive as originally developed in [5], 6\n",
            "their cost has been drastically reduced thanks to shar- 7\n",
            "ing convolutions across proposals [1], [2]. the latest 8\n",
            "incarnation, fast r-cnn [2], achieves near real-time 9\n",
            "rates using very deep networks [3], when ignoring the 10\n",
            "time spent on region proposals. now, proposals are the 11\n",
            "test-time computational bottleneck in state-of-the-art 12\n",
            "detection systems. 13\n",
            "region proposal methods typically rely on inex- 14\n",
            "pensive features and economical inference schemes. 15\n",
            "selective search [4], one of the most popular meth- 16\n",
            "ods, greedily merges superpixels based on engineered 17\n",
            "low-level features. yet when compared to efcient 18\n",
            "detection networks [2], selective search is an order of 19\n",
            "magnitude slower, at 2 seconds per image in a cpu 20\n",
            "implementation. edgeboxes [6] currently provides the 21\n",
            "best tradeoff between proposal quality and speed, 22\n",
            "at 0.2 seconds per image. nevertheless, the region 23\n",
            "proposal step still consumes as much running time 24\n",
            "as the detection network. 25\n",
            "s. ren is with university of science and technology of china, hefei,\n",
            "china. this work was done when s. ren was an intern at microsoft\n",
            "research. email: sqren@mail.ustc.edu.cn\n",
            "k. he and j. sun are with visual computing group, microsoft\n",
            "research. e-mail:fkahe,jiansung@microsoft.com\n",
            "r. girshick is with facebook ai research. the majority of this work\n",
            "was done when r. girshick was with microsoft research. e-mail:\n",
            "rbg@fb.comone may note that fast region-based cnns take 26\n",
            "advantage of gpus, while the region proposal meth- 27\n",
            "ods used in research are implemented on the cpu, 28\n",
            "making such runtime comparisons inequitable. an ob- 29\n",
            "vious way to accelerate proposal computation is to re- 30\n",
            "implement it for the gpu. this may be an effective en- 31\n",
            "gineering solution, but re-implementation ignores the 32\n",
            "down-stream detection network and therefore misses 33\n",
            "important opportunities for sharing computation. 34\n",
            "in this paper, we show that an algorithmic change 35\n",
            "computing proposals with a deep convolutional neu- 36\n",
            "ral networkleads to an elegant and effective solution 37\n",
            "where proposal computation is nearly cost-free given 38\n",
            "the detection networks computation. to this end, we 39\n",
            "introduce novel region proposal networks (rpns) that 40\n",
            "share convolutional layers with state-of-the-art object 41\n",
            "detection networks [1], [2]. by sharing convolutions at 42\n",
            "test-time, the marginal cost for computing proposals 43\n",
            "is small (e.g., 10ms per image). 44\n",
            "our observation is that the convolutional feature 45\n",
            "maps used by region-based detectors, like fast r- 46\n",
            "cnn, can also be used for generating region pro- 47\n",
            "posals. on top of these convolutional features, we 48\n",
            "construct an rpn by adding a few additional con- 49\n",
            "volutional layers that simultaneously regress region 50\n",
            "bounds and objectness scores at each location on a 51\n",
            "regular grid. the rpn is thus a kind of fully convo- 52\n",
            "lutional network (fcn) [7] and can be trained end-to- 53\n",
            "end specically for the task for generating detection 54\n",
            "proposals. 55\n",
            "rpns are designed to efciently predict region pro- 56\n",
            "posals with a wide range of scales and aspect ratios. in 57\n",
            "contrast to prevalent methods [8], [9], [1], [2] that use 58</p></body></html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removimng Stopwords, Punctuations And Extracting Feactures\n"
      ],
      "metadata": {
        "id": "45JDvM_Z6i-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "intro=cleantext\n",
        "\n",
        "pattern1 = r'^\\s*|\\s\\s*'\n",
        "intro=re.sub(pattern1, ' ', str(intro)).strip()\n",
        "\n",
        "intro=intro.lower()\n",
        "\n",
        "print(intro)\n",
        "\n",
        "splchr_pattern = r'[^0-9a-zA-Z ﬁ]' \n",
        "intro=re.sub(splchr_pattern, '', str(intro))\n",
        "\n",
        "num_pattern = r'[0-9]' \n",
        "intro = re.sub(num_pattern, '', str(intro))\n",
        "\n",
        "intro=nltk.word_tokenize(intro)\n",
        "\n",
        "for word in intro:\n",
        "  removed=' '.join([word for word in intro if word not in string.punctuation])\n",
        "intro=removed\n",
        "\n",
        "intro=nlp(intro)\n",
        "intro = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in intro])\n",
        "\n",
        "print(intro)\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "features=[word for word in nltk.word_tokenize(intro) if word not in stopword_list]\n",
        "\n",
        "word_list = set(nltk.corpus.words.words())\n",
        "features=[word for word in features if word in word_list and len(word)>1]\n",
        "\n",
        "print(\"Nubmber of features: \",len(features))\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQIozAUB14AN",
        "outputId": "97a765b5-fdc7-4dcf-e2a5-4a37d39e5d38"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html><body><p>aster r-cnn: towards real-time object detection with region proposal networks shaoqing ren, kaiming he, ross girshick, and jian sun abstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. advances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn into a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with attention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3], our detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco 2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been made publicly available. index termsobject detection, region proposal, convolutional neural network. f 1 i ntroduction 1 recent advances in object detection are driven by 2 the success of region proposal methods (e.g., [4]) 3 and region-based convolutional neural networks (r- 4 cnns) [5]. although region-based cnns were com- 5 putationally expensive as originally developed in [5], 6 their cost has been drastically reduced thanks to shar- 7 ing convolutions across proposals [1], [2]. the latest 8 incarnation, fast r-cnn [2], achieves near real-time 9 rates using very deep networks [3], when ignoring the 10 time spent on region proposals. now, proposals are the 11 test-time computational bottleneck in state-of-the-art 12 detection systems. 13 region proposal methods typically rely on inex- 14 pensive features and economical inference schemes. 15 selective search [4], one of the most popular meth- 16 ods, greedily merges superpixels based on engineered 17 low-level features. yet when compared to efcient 18 detection networks [2], selective search is an order of 19 magnitude slower, at 2 seconds per image in a cpu 20 implementation. edgeboxes [6] currently provides the 21 best tradeoff between proposal quality and speed, 22 at 0.2 seconds per image. nevertheless, the region 23 proposal step still consumes as much running time 24 as the detection network. 25 s. ren is with university of science and technology of china, hefei, china. this work was done when s. ren was an intern at microsoft research. email: sqren@mail.ustc.edu.cn k. he and j. sun are with visual computing group, microsoft research. e-mail:fkahe,jiansung@microsoft.com r. girshick is with facebook ai research. the majority of this work was done when r. girshick was with microsoft research. e-mail: rbg@fb.comone may note that fast region-based cnns take 26 advantage of gpus, while the region proposal meth- 27 ods used in research are implemented on the cpu, 28 making such runtime comparisons inequitable. an ob- 29 vious way to accelerate proposal computation is to re- 30 implement it for the gpu. this may be an effective en- 31 gineering solution, but re-implementation ignores the 32 down-stream detection network and therefore misses 33 important opportunities for sharing computation. 34 in this paper, we show that an algorithmic change 35 computing proposals with a deep convolutional neu- 36 ral networkleads to an elegant and effective solution 37 where proposal computation is nearly cost-free given 38 the detection networks computation. to this end, we 39 introduce novel region proposal networks (rpns) that 40 share convolutional layers with state-of-the-art object 41 detection networks [1], [2]. by sharing convolutions at 42 test-time, the marginal cost for computing proposals 43 is small (e.g., 10ms per image). 44 our observation is that the convolutional feature 45 maps used by region-based detectors, like fast r- 46 cnn, can also be used for generating region pro- 47 posals. on top of these convolutional features, we 48 construct an rpn by adding a few additional con- 49 volutional layers that simultaneously regress region 50 bounds and objectness scores at each location on a 51 regular grid. the rpn is thus a kind of fully convo- 52 lutional network (fcn) [7] and can be trained end-to- 53 end specically for the task for generating detection 54 proposals. 55 rpns are designed to efciently predict region pro- 56 posals with a wide range of scales and aspect ratios. in 57 contrast to prevalent methods [8], [9], [1], [2] that use 580162-8828 (c) 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission. see http://www.ieee.org/publications_standards/publications/rights/index.html for more information.this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication. citation information: doi 10.1109/tpami.2016.2577031, ieee transactions on pattern analysis and machine intelligence 1 faster r-cnn: towards real-time object detection with region proposal networks shaoqing ren, kaiming he, ross girshick, and jian sun abstractstate-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. advances like sppnet [1] and fast r-cnn [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn into a single network by sharing their convolutional featuresusing the recently popular terminology of neural networks with attention mechanisms, the rpn component tells the unied network where to look. for the very deep vgg-16 model [3], our detection system has a frame rate of 5fps (including all steps ) on a gpu, while achieving state-of-the-art object detection accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco 2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been made publicly available. index termsobject detection, region proposal, convolutional neural network. f 1 i ntroduction 1 recent advances in object detection are driven by 2 the success of region proposal methods (e.g., [4]) 3 and region-based convolutional neural networks (r- 4 cnns) [5]. although region-based cnns were com- 5 putationally expensive as originally developed in [5], 6 their cost has been drastically reduced thanks to shar- 7 ing convolutions across proposals [1], [2]. the latest 8 incarnation, fast r-cnn [2], achieves near real-time 9 rates using very deep networks [3], when ignoring the 10 time spent on region proposals. now, proposals are the 11 test-time computational bottleneck in state-of-the-art 12 detection systems. 13 region proposal methods typically rely on inex- 14 pensive features and economical inference schemes. 15 selective search [4], one of the most popular meth- 16 ods, greedily merges superpixels based on engineered 17 low-level features. yet when compared to efcient 18 detection networks [2], selective search is an order of 19 magnitude slower, at 2 seconds per image in a cpu 20 implementation. edgeboxes [6] currently provides the 21 best tradeoff between proposal quality and speed, 22 at 0.2 seconds per image. nevertheless, the region 23 proposal step still consumes as much running time 24 as the detection network. 25 s. ren is with university of science and technology of china, hefei, china. this work was done when s. ren was an intern at microsoft research. email: sqren@mail.ustc.edu.cn k. he and j. sun are with visual computing group, microsoft research. e-mail:fkahe,jiansung@microsoft.com r. girshick is with facebook ai research. the majority of this work was done when r. girshick was with microsoft research. e-mail: rbg@fb.comone may note that fast region-based cnns take 26 advantage of gpus, while the region proposal meth- 27 ods used in research are implemented on the cpu, 28 making such runtime comparisons inequitable. an ob- 29 vious way to accelerate proposal computation is to re- 30 implement it for the gpu. this may be an effective en- 31 gineering solution, but re-implementation ignores the 32 down-stream detection network and therefore misses 33 important opportunities for sharing computation. 34 in this paper, we show that an algorithmic change 35 computing proposals with a deep convolutional neu- 36 ral networkleads to an elegant and effective solution 37 where proposal computation is nearly cost-free given 38 the detection networks computation. to this end, we 39 introduce novel region proposal networks (rpns) that 40 share convolutional layers with state-of-the-art object 41 detection networks [1], [2]. by sharing convolutions at 42 test-time, the marginal cost for computing proposals 43 is small (e.g., 10ms per image). 44 our observation is that the convolutional feature 45 maps used by region-based detectors, like fast r- 46 cnn, can also be used for generating region pro- 47 posals. on top of these convolutional features, we 48 construct an rpn by adding a few additional con- 49 volutional layers that simultaneously regress region 50 bounds and objectness scores at each location on a 51 regular grid. the rpn is thus a kind of fully convo- 52 lutional network (fcn) [7] and can be trained end-to- 53 end specically for the task for generating detection 54 proposals. 55 rpns are designed to efciently predict region pro- 56 posals with a wide range of scales and aspect ratios. in 57 contrast to prevalent methods [8], [9], [1], [2] that use 58</p></body></html>\n",
            "htmlbodypaster rcnn towards realtime object detection with region proposal network shaoqe ren kaime he ross girshick and jian sun abstractstateoftheart object detection network depend on region proposal algorithm to hypothesize object location advance like sppnet and fast rcnn have reduce the run time of these detection network expose region proposal computation as a bottleneck in this work we introduce a region proposal network rpn that share fullimage convolutional feature with the detection network thus enable nearly costfree region proposal an rpn be a fully convolutional network that simultaneously predict object bound and objectness score at each position the rpn be train endtoend to generate highquality region proposal which be use by fast rcnn for detection we far merge rpn and fast rcnn into a single network by share their convolutional featuresuse the recently popular terminology of neural network with attention mechanism the rpn component tell the unied network where to look for the very deep vgg model our detection system have a frame rate of fps include all step on a gpu while achieve stateoftheart object detection accuracy on pascal voc and ms coco dataset with only proposal per image in ilsvrc and coco competition fast rcnn and rpn be the foundation of the stplace win entry in several track code have be make publicly available index termsobject detection region proposal convolutional neural network f i ntroduction recent advance in object detection be drive by the success of region proposal method eg and regionbase convolutional neural network r cnns although regionbase cnn be com putationally expensive as originally develop in their cost have be drastically reduce thank to shar ing convolution across proposal the late incarnation fast rcnn achieve near realtime rate use very deep network when ignore the time spend on region proposal now proposal be the testtime computational bottleneck in stateoftheart detection system region proposal method typically rely on inex pensive feature and economical inference scheme selective search one of the most popular meth ods greedily merge superpixel base on engineer lowlevel feature yet when compare to efcient detection network selective search be an order of magnitude slow at second per image in a cpu implementation edgeboxe currently provide the good tradeoff between proposal quality and speed at second per image nevertheless the region proposal step still consume as much run time as the detection network s ren be with university of science and technology of china hefei china this work be do when s ren be an intern at microsoft research email sqrenmailustceducn k he and j sun be with visual computing group microsoft research emailfkahejiansungmicrosoftcom r girshick be with facebook ai research the majority of this work be do when r girshick be with microsoft research email rbgfbcomone may note that fast regionbase cnn take advantage of gpus while the region proposal meth od use in research be implement on the cpu make such runtime comparison inequitable an ob vious way to accelerate proposal computation be to re implement it for the gpu this may be an effective en gineering solution but reimplementation ignore the downstream detection network and therefore miss important opportunity for share computation in this paper we show that an algorithmic change computing proposal with a deep convolutional neu ral networklead to an elegant and effective solution where proposal computation be nearly costfree give the detection network computation to this end we introduce novel region proposal network rpns that share convolutional layer with stateoftheart object detection network by share convolution at testtime the marginal cost for computing proposal be small eg ms per image our observation be that the convolutional feature map use by regionbase detector like fast r cnn can also be use for generate region pro posal on top of these convolutional feature we construct an rpn by add a few additional con volutional layer that simultaneously regress region bound and objectness score at each location on a regular grid the rpn be thus a kind of fully convo lutional network fcn and can be train endto end specically for the task for generate detection proposal rpns be design to efciently predict region pro posal with a wide range of scale and aspect ratio in contrast to prevalent method that use c ieee personal use be permit but republicationredistribution require ieee permission see httpwwwieeeorgpublicationsstandardspublicationsrightsindexhtml for more informationthis article have be accept for publication in a future issue of this journal but have not be fully edit content may change prior to final publication citation information doi tpami ieee transaction on pattern analysis and machine intelligence fast rcnn towards realtime object detection with region proposal network shaoqe ren kaime he ross girshick and jian sun abstractstateoftheart object detection network depend on region proposal algorithm to hypothesize object location advance like sppnet and fast rcnn have reduce the run time of these detection network expose region proposal computation as a bottleneck in this work we introduce a region proposal network rpn that share fullimage convolutional feature with the detection network thus enable nearly costfree region proposal an rpn be a fully convolutional network that simultaneously predict object bound and objectness score at each position the rpn be train endtoend to generate highquality region proposal which be use by fast rcnn for detection we far merge rpn and fast rcnn into a single network by share their convolutional featuresuse the recently popular terminology of neural network with attention mechanism the rpn component tell the unied network where to look for the very deep vgg model our detection system have a frame rate of fps include all step on a gpu while achieve stateoftheart object detection accuracy on pascal voc and ms coco dataset with only proposal per image in ilsvrc and coco competition fast rcnn and rpn be the foundation of the stplace win entry in several track code have be make publicly available index termsobject detection region proposal convolutional neural network f i ntroduction recent advance in object detection be drive by the success of region proposal method eg and regionbase convolutional neural network r cnns although regionbase cnn be com putationally expensive as originally develop in their cost have be drastically reduce thank to shar ing convolution across proposal the late incarnation fast rcnn achieve near realtime rate use very deep network when ignore the time spend on region proposal now proposal be the testtime computational bottleneck in stateoftheart detection system region proposal method typically rely on inex pensive feature and economical inference scheme selective search one of the most popular meth ods greedily merge superpixel base on engineer lowlevel feature yet when compare to efcient detection network selective search be an order of magnitude slow at second per image in a cpu implementation edgeboxe currently provide the good tradeoff between proposal quality and speed at second per image nevertheless the region proposal step still consume as much run time as the detection network s ren be with university of science and technology of china hefei china this work be do when s ren be an intern at microsoft research email sqrenmailustceducn k he and j sun be with visual computing group microsoft research emailfkahejiansungmicrosoftcom r girshick be with facebook ai research the majority of this work be do when r girshick be with microsoft research email rbgfbcomone may note that fast regionbase cnn take advantage of gpus while the region proposal meth od use in research be implement on the cpu make such runtime comparison inequitable an ob vious way to accelerate proposal computation be to re implement it for the gpu this may be an effective en gineering solution but reimplementation ignore the downstream detection network and therefore miss important opportunity for share computation in this paper we show that an algorithmic change computing proposal with a deep convolutional neu ral networklead to an elegant and effective solution where proposal computation be nearly costfree give the detection network computation to this end we introduce novel region proposal network rpns that share convolutional layer with stateoftheart object detection network by share convolution at testtime the marginal cost for computing proposal be small eg ms per image our observation be that the convolutional feature map use by regionbase detector like fast r cnn can also be use for generate region pro posal on top of these convolutional feature we construct an rpn by add a few additional con volutional layer that simultaneously regress region bound and objectness score at each location on a regular grid the rpn be thus a kind of fully convo lutional network fcn and can be train endto end specically for the task for generate detection proposal rpns be design to efciently predict region pro posal with a wide range of scale and aspect ratio in contrast to prevalent method that use pbodyhtml\n",
            "Nubmber of features:  726\n",
            "['towards', 'object', 'detection', 'region', 'proposal', 'network', 'ross', 'sun', 'object', 'detection', 'network', 'depend', 'region', 'proposal', 'algorithm', 'hypothesize', 'object', 'location', 'advance', 'like', 'fast', 'reduce', 'run', 'time', 'detection', 'network', 'expose', 'region', 'proposal', 'computation', 'bottleneck', 'work', 'introduce', 'region', 'proposal', 'network', 'share', 'convolutional', 'feature', 'detection', 'network', 'thus', 'enable', 'nearly', 'region', 'proposal', 'fully', 'convolutional', 'network', 'simultaneously', 'predict', 'object', 'bound', 'score', 'position', 'train', 'generate', 'region', 'proposal', 'use', 'fast', 'detection', 'far', 'merge', 'fast', 'single', 'network', 'share', 'convolutional', 'recently', 'popular', 'terminology', 'neural', 'network', 'attention', 'mechanism', 'component', 'tell', 'network', 'look', 'deep', 'model', 'detection', 'system', 'frame', 'rate', 'include', 'step', 'achieve', 'object', 'detection', 'accuracy', 'coco', 'proposal', 'per', 'image', 'coco', 'competition', 'fast', 'foundation', 'win', 'entry', 'several', 'track', 'code', 'make', 'publicly', 'available', 'index', 'detection', 'region', 'proposal', 'convolutional', 'neural', 'network', 'recent', 'advance', 'object', 'detection', 'drive', 'success', 'region', 'proposal', 'method', 'convolutional', 'neural', 'network', 'although', 'expensive', 'originally', 'develop', 'cost', 'drastically', 'reduce', 'thank', 'ing', 'convolution', 'across', 'proposal', 'late', 'incarnation', 'fast', 'achieve', 'near', 'rate', 'use', 'deep', 'network', 'ignore', 'time', 'spend', 'region', 'proposal', 'proposal', 'computational', 'bottleneck', 'detection', 'system', 'region', 'proposal', 'method', 'typically', 'rely', 'pensive', 'feature', 'economical', 'inference', 'scheme', 'selective', 'search', 'one', 'popular', 'greedily', 'merge', 'base', 'engineer', 'feature', 'yet', 'compare', 'detection', 'network', 'selective', 'search', 'order', 'magnitude', 'slow', 'second', 'per', 'image', 'implementation', 'currently', 'provide', 'good', 'proposal', 'quality', 'speed', 'second', 'per', 'image', 'nevertheless', 'region', 'proposal', 'step', 'still', 'consume', 'much', 'run', 'time', 'detection', 'network', 'university', 'science', 'technology', 'china', 'china', 'work', 'intern', 'research', 'sun', 'visual', 'group', 'research', 'ai', 'research', 'majority', 'work', 'research', 'may', 'note', 'fast', 'take', 'advantage', 'region', 'proposal', 'od', 'use', 'research', 'implement', 'make', 'comparison', 'inequitable', 'way', 'accelerate', 'proposal', 'computation', 'implement', 'may', 'effective', 'en', 'solution', 'ignore', 'downstream', 'detection', 'network', 'therefore', 'miss', 'important', 'opportunity', 'share', 'computation', 'paper', 'show', 'algorithmic', 'change', 'proposal', 'deep', 'convolutional', 'elegant', 'effective', 'solution', 'proposal', 'computation', 'nearly', 'give', 'detection', 'network', 'computation', 'end', 'introduce', 'novel', 'region', 'proposal', 'network', 'share', 'convolutional', 'layer', 'object', 'detection', 'network', 'share', 'convolution', 'marginal', 'cost', 'proposal', 'small', 'per', 'image', 'observation', 'convolutional', 'feature', 'map', 'use', 'detector', 'like', 'fast', 'also', 'use', 'generate', 'region', 'pro', 'top', 'convolutional', 'feature', 'construct', 'add', 'additional', 'con', 'layer', 'simultaneously', 'regress', 'region', 'bound', 'score', 'location', 'regular', 'grid', 'thus', 'kind', 'fully', 'network', 'train', 'end', 'task', 'generate', 'detection', 'proposal', 'design', 'predict', 'region', 'pro', 'wide', 'range', 'scale', 'aspect', 'ratio', 'contrast', 'prevalent', 'method', 'use', 'personal', 'use', 'permit', 'require', 'permission', 'see', 'article', 'accept', 'publication', 'future', 'issue', 'journal', 'fully', 'edit', 'content', 'may', 'change', 'prior', 'final', 'publication', 'citation', 'information', 'transaction', 'pattern', 'analysis', 'machine', 'intelligence', 'fast', 'towards', 'object', 'detection', 'region', 'proposal', 'network', 'ross', 'sun', 'object', 'detection', 'network', 'depend', 'region', 'proposal', 'algorithm', 'hypothesize', 'object', 'location', 'advance', 'like', 'fast', 'reduce', 'run', 'time', 'detection', 'network', 'expose', 'region', 'proposal', 'computation', 'bottleneck', 'work', 'introduce', 'region', 'proposal', 'network', 'share', 'convolutional', 'feature', 'detection', 'network', 'thus', 'enable', 'nearly', 'region', 'proposal', 'fully', 'convolutional', 'network', 'simultaneously', 'predict', 'object', 'bound', 'score', 'position', 'train', 'generate', 'region', 'proposal', 'use', 'fast', 'detection', 'far', 'merge', 'fast', 'single', 'network', 'share', 'convolutional', 'recently', 'popular', 'terminology', 'neural', 'network', 'attention', 'mechanism', 'component', 'tell', 'network', 'look', 'deep', 'model', 'detection', 'system', 'frame', 'rate', 'include', 'step', 'achieve', 'object', 'detection', 'accuracy', 'coco', 'proposal', 'per', 'image', 'coco', 'competition', 'fast', 'foundation', 'win', 'entry', 'several', 'track', 'code', 'make', 'publicly', 'available', 'index', 'detection', 'region', 'proposal', 'convolutional', 'neural', 'network', 'recent', 'advance', 'object', 'detection', 'drive', 'success', 'region', 'proposal', 'method', 'convolutional', 'neural', 'network', 'although', 'expensive', 'originally', 'develop', 'cost', 'drastically', 'reduce', 'thank', 'ing', 'convolution', 'across', 'proposal', 'late', 'incarnation', 'fast', 'achieve', 'near', 'rate', 'use', 'deep', 'network', 'ignore', 'time', 'spend', 'region', 'proposal', 'proposal', 'computational', 'bottleneck', 'detection', 'system', 'region', 'proposal', 'method', 'typically', 'rely', 'pensive', 'feature', 'economical', 'inference', 'scheme', 'selective', 'search', 'one', 'popular', 'greedily', 'merge', 'base', 'engineer', 'feature', 'yet', 'compare', 'detection', 'network', 'selective', 'search', 'order', 'magnitude', 'slow', 'second', 'per', 'image', 'implementation', 'currently', 'provide', 'good', 'proposal', 'quality', 'speed', 'second', 'per', 'image', 'nevertheless', 'region', 'proposal', 'step', 'still', 'consume', 'much', 'run', 'time', 'detection', 'network', 'university', 'science', 'technology', 'china', 'china', 'work', 'intern', 'research', 'sun', 'visual', 'group', 'research', 'ai', 'research', 'majority', 'work', 'research', 'may', 'note', 'fast', 'take', 'advantage', 'region', 'proposal', 'od', 'use', 'research', 'implement', 'make', 'comparison', 'inequitable', 'way', 'accelerate', 'proposal', 'computation', 'implement', 'may', 'effective', 'en', 'solution', 'ignore', 'downstream', 'detection', 'network', 'therefore', 'miss', 'important', 'opportunity', 'share', 'computation', 'paper', 'show', 'algorithmic', 'change', 'proposal', 'deep', 'convolutional', 'elegant', 'effective', 'solution', 'proposal', 'computation', 'nearly', 'give', 'detection', 'network', 'computation', 'end', 'introduce', 'novel', 'region', 'proposal', 'network', 'share', 'convolutional', 'layer', 'object', 'detection', 'network', 'share', 'convolution', 'marginal', 'cost', 'proposal', 'small', 'per', 'image', 'observation', 'convolutional', 'feature', 'map', 'use', 'detector', 'like', 'fast', 'also', 'use', 'generate', 'region', 'pro', 'top', 'convolutional', 'feature', 'construct', 'add', 'additional', 'con', 'layer', 'simultaneously', 'regress', 'region', 'bound', 'score', 'location', 'regular', 'grid', 'thus', 'kind', 'fully', 'network', 'train', 'end', 'task', 'generate', 'detection', 'proposal', 'design', 'predict', 'region', 'pro', 'wide', 'range', 'scale', 'aspect', 'ratio', 'contrast', 'prevalent', 'method', 'use']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Conclusion:  Successfully extracted the introduction part from the given pdf and performed\n",
        "data preprocessing and feacture engineering."
      ],
      "metadata": {
        "id": "DoEJeSW_59v9"
      }
    }
  ]
}